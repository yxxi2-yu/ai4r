{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_b2dNRBjADs"
      },
      "source": [
        "## Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kcdeMTmr1l-S",
        "outputId": "8ba22404-7389-4b4f-e211-c075da21e9f2"
      },
      "outputs": [],
      "source": [
        "!pip install -q gymnasium stable_baselines3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4qASuf1o1IFk",
        "outputId": "45beacbe-b976-46f2-c996-b89bcb143310"
      },
      "outputs": [],
      "source": [
        "!git clone https://gitlab.unimelb.edu.au/ai4r/ai4r-gym.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vioqx1Rb1_PY",
        "outputId": "6083b23f-e907-4fa2-b242-4a8e8a4465ea"
      },
      "outputs": [],
      "source": [
        "%cd ai4r-gym"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yFKScohkPqn6"
      },
      "source": [
        "## Library Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vMrhAAlE2ila"
      },
      "outputs": [],
      "source": [
        "import ai4rgym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import gymnasium as gym\n",
        "from stable_baselines3 import PPO, SAC, DDPG\n",
        "from utils import ensure_dir, ensure_dirs, eval_model, evaluate_policy\n",
        "from utils import plot_rewards, plot_and_animate_trajectory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rcgPKmEPPnCS"
      },
      "source": [
        "## Environment Settings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KdUyqNCx2dvg"
      },
      "source": [
        "### Specify the Vehicle Parameters\n",
        "\n",
        "Dictionary with car specifications,\n",
        "in the form of a dynamic bicycle model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PFXr8MMT2C3Y",
        "outputId": "f9490218-95e6-42e3-e5cd-0172895b6d1a"
      },
      "outputs": [],
      "source": [
        "# SPECIFY THE VEHCILE PARAMETERS\n",
        "bicycle_model_parameters = {\n",
        "    \"Lf\" : 0.55*2.875,\n",
        "    \"Lr\" : 0.45*2.875,\n",
        "    \"m\"  : 2000.0,\n",
        "    \"Iz\" : (1.0/12.0) * 2000.0 * (4.692**2+1.850**2),\n",
        "    \"Cm\" : (1.0/100.0) * (1.0 * 400.0 * 9.0) / 0.2286,\n",
        "    \"Cd\" : 0.5 * 0.24 * 2.2204 * 1.202,\n",
        "    \"delta_offset\" : 0 * np.pi/180,\n",
        "    \"delta_request_max\" : 45 * np.pi/180,\n",
        "    \"Ddelta_lower_limit\" : -45 * np.pi/180,\n",
        "    \"Ddelta_upper_limit\" :  45 * np.pi/180,\n",
        "    \"v_transition_min\" : 500.0 / 3.6,\n",
        "    \"v_transition_max\" : 600.0 / 3.6,\n",
        "    \"body_len_f\" : (0.55*2.875) * 1.5,\n",
        "    \"body_len_r\" : (0.45*2.875) * 1.5,\n",
        "    \"body_width\" : 2.50,\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZvLBp0Y2oca"
      },
      "source": [
        "### Specify the Road\n",
        "\n",
        "\n",
        "Specified as a list of dictionaries, where each\n",
        "element in the list specifies a segment of the road.\n",
        "Example segment dictionaries:\n",
        "```Python\n",
        "{\"type\":\"straight\", \"length\":3.0}\n",
        "{\"type\":\"curved\", \"curvature\":1/50.0, \"angle_in_degrees\":45.0}\n",
        "{\"type\":\"curved\", \"curvature\":1/50.0, \"length\":30.0}\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TCXqw6rF2Cz1"
      },
      "outputs": [],
      "source": [
        "road_elements_list = [\n",
        "    {\"type\":\"straight\", \"length\":100.0, \"v_max_kph\":60.0},\n",
        "    {\"type\":\"curved\", \"curvature\":1/800.0, \"angle_in_degrees\":15.0, \"v_max_kph\":50.0},\n",
        "    {\"type\":\"straight\", \"length\":100.0, \"v_max_kph\":60.0},\n",
        "    {\"type\":\"curved\", \"curvature\":-1/400.0, \"angle_in_degrees\":30.0, \"v_max_kph\":40.0},\n",
        "    {\"type\":\"straight\", \"length\":100.0, \"v_max_kph\":60.0},\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-Fn5qyh2r69"
      },
      "source": [
        "### Specify the Numerical Integration Details\n",
        "\n",
        "The options available for the numerical\n",
        "integration method are:\n",
        "`[\"euler\", \"huen\", \"midpoint\", \"rk4\", \"rk45\"]`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2e5yTWLP2CwX"
      },
      "outputs": [],
      "source": [
        "numerical_integration_parameters = {\n",
        "    \"method\" : \"rk4\",\n",
        "    \"Ts\" : 0.05,\n",
        "    \"num_steps_per_Ts\" : 1,\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4j-7ygm2xLw"
      },
      "source": [
        "###  Specify the Initial State Distribution\n",
        "\n",
        "The initial state is sampled from a uniform\n",
        "distribution between the minimum and maximum\n",
        "(i.e., between lower and upper bounds)\n",
        "> Note: a factor of (1/3.6) converts from units of [km/h] to [m/s]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xFOpyqMd2xkG"
      },
      "outputs": [],
      "source": [
        "py_init_min = -1.0\n",
        "py_init_max =  1.0\n",
        "\n",
        "v_init_min_in_kmh = 55.0\n",
        "v_init_max_in_kmh = 65.0\n",
        "\n",
        "py_init_min = -1.0\n",
        "py_init_max =  1.0\n",
        "\n",
        "v_init_min_in_kmh = 55.0\n",
        "v_init_max_in_kmh = 65.0\n",
        "\n",
        "initial_state_bounds = {\n",
        "    \"px_init_min\" : 0.0,\n",
        "    \"px_init_max\" : 0.0,\n",
        "    \"py_init_min\" : py_init_min,\n",
        "    \"py_init_max\" : py_init_max,\n",
        "    \"theta_init_min\" : 0.0,\n",
        "    \"theta_init_max\" : 0.0,\n",
        "    \"vx_init_min\" : v_init_min_in_kmh * (1.0/3.6),\n",
        "    \"vx_init_max\" : v_init_max_in_kmh * (1.0/3.6),\n",
        "    \"vy_init_min\" : 0.0,\n",
        "    \"vy_init_max\" : 0.0,\n",
        "    \"omega_init_min\" : 0.0,\n",
        "    \"omega_init_max\" : 0.0,\n",
        "    \"delta_init_min\" : 0.0,\n",
        "    \"delta_init_max\" : 0.0,\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UB-aBfdd20iz"
      },
      "source": [
        "### Specify the Observation Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WdTlLQE36PUX"
      },
      "outputs": [],
      "source": [
        "# SPECIFY THE OBSERVATION PARAMETERS\n",
        "observation_parameters = {\n",
        "    \"should_include_ground_truth_px\"                       :  \"info\",\n",
        "    \"should_include_ground_truth_py\"                       :  \"info\",\n",
        "    \"should_include_ground_truth_theta\"                    :  \"info\",\n",
        "    \"should_include_ground_truth_vx\"                       :  \"info\",\n",
        "    \"should_include_ground_truth_vy\"                       :  \"info\",\n",
        "    \"should_include_ground_truth_omega\"                    :  \"info\",\n",
        "    \"should_include_ground_truth_delta\"                    :  \"info\",\n",
        "    \"should_include_road_progress_at_closest_point\"        :  \"info\",\n",
        "    \"should_include_vx_sensor\"                             :  \"info\",\n",
        "    \"should_include_distance_to_closest_point\"             :  \"obs\",\n",
        "    \"should_include_heading_angle_relative_to_line\"        :  \"obs\",\n",
        "    \"should_include_heading_angular_rate_gyro\"             :  \"info\",\n",
        "    \"should_include_closest_point_coords_in_body_frame\"    :  \"info\",\n",
        "    \"should_include_look_ahead_line_coords_in_body_frame\"  :  \"info\",\n",
        "    \"should_include_road_curvature_at_closest_point\"       :  \"obs\",\n",
        "    \"should_include_look_ahead_road_curvatures\"            :  \"info\",\n",
        "    \"should_include_speed_limit\"                           :  \"obs\",\n",
        "    \"should_include_recommended_speed\"                     :  \"obs\",\n",
        "    \n",
        "\n",
        "    \"scaling_for_ground_truth_px\"                       :  1.0,\n",
        "    \"scaling_for_ground_truth_py\"                       :  1.0,\n",
        "    \"scaling_for_ground_truth_theta\"                    :  1.0,\n",
        "    \"scaling_for_ground_truth_vx\"                       :  1.0,\n",
        "    \"scaling_for_ground_truth_vy\"                       :  1.0,\n",
        "    \"scaling_for_ground_truth_omega\"                    :  1.0,\n",
        "    \"scaling_for_ground_truth_delta\"                    :  1.0,\n",
        "    \"scaling_for_road_progress_at_closest_point\"        :  1.0,\n",
        "    \"scaling_for_vx_sensor\"                             :  1.0,\n",
        "    \"scaling_for_distance_to_closest_point\"             :  1.0,\n",
        "    \"scaling_for_heading_angle_relative_to_line\"        :  1.0,\n",
        "    \"scaling_for_heading_angular_rate_gyro\"             :  1.0,\n",
        "    \"scaling_for_closest_point_coords_in_body_frame\"    :  1.0,\n",
        "    \"scaling_for_look_ahead_line_coords_in_body_frame\"  :  1.0,\n",
        "    \"scaling_for_road_curvature_at_closest_point\"       :  1.0,\n",
        "    \"scaling_for_look_ahead_road_curvatures\"            :  1.0,\n",
        "    \"scaling_for_speed_limit\"                           :  1.0,\n",
        "    \"scaling_for_recommended_speed\"                     :  1.0,\n",
        "\n",
        "    \"vx_sensor_bias\"    : 0.0,\n",
        "    \"vx_sensor_stddev\"  : 0.1,\n",
        "\n",
        "    \"distance_to_closest_point_bias\"    :  0.0,\n",
        "    \"distance_to_closest_point_stddev\"  :  0.01,\n",
        "\n",
        "    \"heading_angle_relative_to_line_bias\"    :  0.0,\n",
        "    \"heading_angle_relative_to_line_stddev\"  :  0.01,\n",
        "\n",
        "    \"heading_angular_rate_gyro_bias\"    :  0.0,\n",
        "    \"heading_angular_rate_gyro_stddev\"  :  0.01,\n",
        "\n",
        "    \"closest_point_coords_in_body_frame_bias\"    :  0.0,\n",
        "    \"closest_point_coords_in_body_frame_stddev\"  :  0.0,\n",
        "\n",
        "    \"look_ahead_line_coords_in_body_frame_bias\"    :  0.0,\n",
        "    \"look_ahead_line_coords_in_body_frame_stddev\"  :  0.0,\n",
        "\n",
        "    \"road_curvature_at_closest_point_bias\"    :  0.0,\n",
        "    \"road_curvature_at_closest_point_stddev\"  :  0.0,\n",
        "\n",
        "    \"look_ahead_road_curvatures_bias\"    :  0.0,\n",
        "    \"look_ahead_road_curvatures_stddev\"  :  0.0,\n",
        "\n",
        "    \"look_ahead_line_coords_in_body_frame_distance\"    :  100.0,\n",
        "    \"look_ahead_line_coords_in_body_frame_num_points\"  :  10,\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3d4y71xEJZP"
      },
      "source": [
        "### Specify Termination Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pLnYuvG9EJKC"
      },
      "outputs": [],
      "source": [
        "termination_parameters = {\n",
        "    \"speed_lower_bound\"  :  0.0,\n",
        "    \"speed_upper_bound\"  :  (200.0/3.6),\n",
        "    \"distance_to_closest_point_upper_bound\"  :  20.0,\n",
        "    \"reward_speed_lower_bound\"  :  0.0,\n",
        "    \"reward_speed_upper_bound\"  :  0.0,\n",
        "    \"reward_distance_to_closest_point_upper_bound\"  :  0.0,\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gvm3nDWDQo_S"
      },
      "source": [
        "## Environment Definition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvkR3qZm27ua"
      },
      "source": [
        "### Initialise the Autonomous Driving Environment\n",
        "\n",
        "Options available for the \"render_mode\" are:\n",
        "`[\"matplotlib\", None]`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tCAOQ0AE25iF"
      },
      "outputs": [],
      "source": [
        "env = gym.make(\n",
        "    \"ai4rgym/autonomous_driving_env\",\n",
        "    render_mode=None,\n",
        "    bicycle_model_parameters=bicycle_model_parameters,\n",
        "    road_elements_list=road_elements_list,\n",
        "    numerical_integration_parameters=numerical_integration_parameters,\n",
        "    termination_parameters=termination_parameters,\n",
        "    initial_state_bounds=initial_state_bounds,\n",
        "    observation_parameters=observation_parameters,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhoYSGEW3AJ1"
      },
      "source": [
        "### Additional Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qPVqdwYT29qR"
      },
      "outputs": [],
      "source": [
        "# > Time increment per simulation step (units: seconds)\n",
        "Ts_sim = 0.05\n",
        "\n",
        "# Specify the integration method to simulate\n",
        "integration_method = \"rk4\"\n",
        "\n",
        "# Set the integration method and Ts of the gymnasium\n",
        "env.unwrapped.set_integration_method(integration_method)\n",
        "env.unwrapped.set_integration_Ts(Ts_sim)\n",
        "# Set the road condition\n",
        "env.unwrapped.set_road_condition(road_condition=\"wet\")\n",
        "\n",
        "env = gym.wrappers.RescaleAction(env, min_action=-1, max_action=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqEiUpY5QuTj"
      },
      "source": [
        "### Verify Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3gdnT5i33Bhw"
      },
      "outputs": [],
      "source": [
        "# Reset the gymnasium\n",
        "# > which also returns the first observation\n",
        "observation, info_dict = env.reset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zchNh_TL3DOE"
      },
      "outputs": [],
      "source": [
        "observation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eI-M55-z3EPz"
      },
      "outputs": [],
      "source": [
        "info_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VGAzcpLL3GfO"
      },
      "outputs": [],
      "source": [
        "env.observation_space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "34cB5qYp7SZG"
      },
      "outputs": [],
      "source": [
        "env.action_space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wVyWdy607UTe"
      },
      "outputs": [],
      "source": [
        "random_action = env.action_space.sample()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "guAJPyam7ZiE"
      },
      "outputs": [],
      "source": [
        "observation, reward, terminated, truncated, info = env.step(random_action)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pcn-AKpv7dWj"
      },
      "outputs": [],
      "source": [
        "observation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xaTK3udm7eBb"
      },
      "outputs": [],
      "source": [
        "reward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T6sVR-Cj7euG"
      },
      "outputs": [],
      "source": [
        "info"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9C8Ub1D8nNB"
      },
      "source": [
        "### Reward Wrapper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OGlRTmON8mcL"
      },
      "outputs": [],
      "source": [
        "from typing import Optional\n",
        "\n",
        "\n",
        "class CustomRewardWrapper(gym.Wrapper):\n",
        "    \"\"\"\n",
        "    A custom wrapper that reconstructs the environment's previous reward logic\n",
        "    (progress, deviation from line, speed shaping, and termination rewards)\n",
        "    while the base environment returns zero reward. This keeps reward shaping\n",
        "    out of the env and makes it configurable/replaceable.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, env: gym.Env, cfg: Optional[dict] = None):\n",
        "        super().__init__(env)\n",
        "        self.cfg = cfg or {}\n",
        "        # Coefficients for previous env reward composition\n",
        "        # Previous env used: 0.0*progress + 100.0*deviation + 1.0*speed + termination\n",
        "        # Maintain same defaults here; allow override via cfg\n",
        "        self.k_progress = self.cfg.get(\"k_progress\", 0.0)\n",
        "        self.k_deviation = self.cfg.get(\"k_deviation\", 100.0)\n",
        "        self.k_speed = self.cfg.get(\"k_speed\", 1.0)\n",
        "        # Bonus when finished (not present previously, default 0)\n",
        "        self.finished_bonus = self.cfg.get(\"finished_bonus\", 0.0)\n",
        "        # Timeout penalty if a TimeLimit truncation occurs\n",
        "        self.timeout_penalty = self.cfg.get(\"timeout_penalty\", 0.0)\n",
        "\n",
        "    # Standalone re-implementation of previous reward shaping terms\n",
        "    @staticmethod\n",
        "    def reward_for_distance_to_line(d: float) -> float:\n",
        "        \"\"\"\n",
        "        Mirror AutonomousDrivingEnv.compute_default_reward_for_distance_to_line\n",
        "        d >= 0: distance magnitude to center line.\n",
        "        \"\"\"\n",
        "        a = 3.0\n",
        "        b = 1.0\n",
        "        c = 2.0\n",
        "        if d < 0.5:\n",
        "            return a * (1 - d**2)\n",
        "        elif 0.5 <= d < 2:\n",
        "            return b * (2 - d) ** 2\n",
        "        else:\n",
        "            return -c * (d - 2) ** 3\n",
        "\n",
        "    @staticmethod\n",
        "    def reward_for_speed(speed_mps: float, recommended_speed_mps: float) -> float:\n",
        "        \"\"\"\n",
        "        Quadratic speed reward centered on the road’s recommended speed.\n",
        "\n",
        "        - Inputs and output use SI units (m/s).\n",
        "        - Returns a high reward near the recommended speed and decreases\n",
        "          quadratically as the measured speed deviates from it.\n",
        "        \n",
        "        NOTE: you could optionally factor in the speed limit to apply \n",
        "        an additional penalty for exceeding it.\n",
        "        \"\"\"\n",
        "        # If no recommendation is available, return neutral reward (0.0)\n",
        "        if not (recommended_speed_mps is not None) or recommended_speed_mps <= 0:\n",
        "            return 0.0\n",
        "        a = 6.0 / 1000.0  # ≈ 0.006, a simple fraction that works well\n",
        "        b = 300.0\n",
        "        dv = float(speed_mps) - float(recommended_speed_mps)\n",
        "        return -a * (dv ** 2) + b\n",
        "\n",
        "    def step(self, action):\n",
        "        obs, base_r, terminated, truncated, info = self.env.step(action)\n",
        "        # Base env reward is intentionally zero; we reconstruct externally\n",
        "        r = 0.0\n",
        "\n",
        "        # We need values used by the original env reward terms.\n",
        "        # These are available via env.current_ground_truth and the car state.\n",
        "        # Access safely; if missing, default to zeros.\n",
        "        gt = getattr(self.env, \"current_ground_truth\", {}) or {}\n",
        "        progress_at_closest_point = float(gt.get(\"road_progress_at_closest_point\", 0.0))\n",
        "        distance_to_closest_point = float(abs(gt.get(\"distance_to_closest_point\", 0.0)))\n",
        "\n",
        "        # The env used previous_progress_at_closest_p internally; in the wrapper\n",
        "        # we track our own previous progress to compute progress_reward.\n",
        "        if not hasattr(self, \"_prev_progress\"):\n",
        "            self._prev_progress = progress_at_closest_point\n",
        "        progress_reward = progress_at_closest_point - self._prev_progress\n",
        "        self._prev_progress = progress_at_closest_point\n",
        "\n",
        "        # Speed shaping (m/s) relative to road recommended speed (m/s)\n",
        "        vx_mps = float(getattr(getattr(self.env, \"car\", None), \"vx\", 0.0))\n",
        "        recommended_speed_mps = float(gt.get(\"recommended_speed_at_closest_point\", 0.0))\n",
        "        # recommended_speed_mps = 60.0 / 3.6\n",
        "\n",
        "        deviation_reward = self.reward_for_distance_to_line(distance_to_closest_point)\n",
        "        speed_reward = self.reward_for_speed(vx_mps, recommended_speed_mps)\n",
        "\n",
        "        r += self.k_progress * progress_reward\n",
        "        r += self.k_deviation * deviation_reward\n",
        "        r += self.k_speed * speed_reward\n",
        "\n",
        "        # Termination-based rewards/penalties\n",
        "        term = info.get(\"termination\", {}) if isinstance(info, dict) else {}\n",
        "        term_cfg = info.get(\"termination_rewards\", {}) if isinstance(info, dict) else {}\n",
        "        if terminated:\n",
        "            if term.get(\"speed_high\"):\n",
        "                r += float(term_cfg.get(\"speed_upper_bound\", 0.0))\n",
        "            if term.get(\"speed_low\"):\n",
        "                r += float(term_cfg.get(\"speed_lower_bound\", 0.0))\n",
        "            if term.get(\"off_track\"):\n",
        "                r += float(term_cfg.get(\"off_track\", 0.0))\n",
        "            if term.get(\"finished\"):\n",
        "                r += float(self.finished_bonus)\n",
        "\n",
        "        # Time-limit truncation (Gymnasium's TimeLimit sets this flag in info)\n",
        "        if truncated and isinstance(info, dict) and info.get(\"TimeLimit.truncated\", False):\n",
        "            r += float(self.timeout_penalty)\n",
        "\n",
        "        return obs, r, terminated, truncated, info\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        res = self.env.reset(**kwargs)\n",
        "        # Reset wrapper progress tracker\n",
        "        self._prev_progress = 0.0\n",
        "        # After reset, try to align with current env ground-truth if available\n",
        "        gt = getattr(self.env, \"current_ground_truth\", {}) or {}\n",
        "        self._prev_progress = float(gt.get(\"road_progress_at_closest_point\", 0.0))\n",
        "        return res"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "se7pfJLC9gVD"
      },
      "source": [
        "Create a new environment and wrap it with the Reward Wrapper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VPVyjOle9Wx-"
      },
      "outputs": [],
      "source": [
        "env = gym.make(\n",
        "    \"ai4rgym/autonomous_driving_env\",\n",
        "    render_mode=None,\n",
        "    bicycle_model_parameters=bicycle_model_parameters,\n",
        "    road_elements_list=road_elements_list,\n",
        "    numerical_integration_parameters=numerical_integration_parameters,\n",
        "    termination_parameters=termination_parameters,\n",
        "    initial_state_bounds=initial_state_bounds,\n",
        "    observation_parameters=observation_parameters,\n",
        ")\n",
        "\n",
        "# > Time increment per simulation step (units: seconds)\n",
        "Ts_sim = 0.05\n",
        "\n",
        "# Specify the integration method to simulate\n",
        "integration_method = \"rk4\"\n",
        "\n",
        "# Set the integration method and Ts of the gymnasium\n",
        "env.unwrapped.set_integration_method(integration_method)\n",
        "env.unwrapped.set_integration_Ts(Ts_sim)\n",
        "# Set the road condition\n",
        "env.unwrapped.set_road_condition(road_condition=\"wet\")\n",
        "\n",
        "env = gym.wrappers.RescaleAction(env, min_action=-1, max_action=1)\n",
        "\n",
        "cfg = { 'k_deviation': 100.0, 'k_speed': 1.0}\n",
        "env = CustomRewardWrapper(env, cfg=cfg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wg-Q9g7i9rok"
      },
      "source": [
        "Let's check the reward now:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RftxpP3B9qtD"
      },
      "outputs": [],
      "source": [
        "observation, info = env.reset()\n",
        "random_action = env.action_space.sample()\n",
        "observation, reward, terminated, truncated, info = env.step(random_action)\n",
        "print(reward)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yo2aNmR3aF60"
      },
      "source": [
        "## Model Definition and Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CV13EDTOGlEU"
      },
      "outputs": [],
      "source": [
        "model_name = \"PPO_SL\"\n",
        "\n",
        "TIMESTEPS_PER_EPOCH = 50000\n",
        "N_EPOCHS = 10\n",
        "\n",
        "logdir = \"logs\"\n",
        "models_dir = f\"models/{model_name}\"\n",
        "figs_dir = f\"models/{model_name}/figs\"\n",
        "\n",
        "ensure_dirs([logdir, models_dir, figs_dir])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DSIjEgi87neh",
        "outputId": "c393752a-b03e-460a-84cb-4b14b3db9f88"
      },
      "outputs": [],
      "source": [
        "model = PPO(\"MultiInputPolicy\", env, verbose = 1)\n",
        "#model = SAC(\"MultiInputPolicy\", env, verbose = 1)\n",
        "#model = DDPG(\"MultiInputPolicy\", env, verbose = 1)\n",
        "#model = TD3(\"MultiInputPolicy\", env, verbose = 1)\n",
        "#model = A2C(\"MultiInputPolicy\", env, verbose = 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YIwe4p1SPh6U"
      },
      "source": [
        "## Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "pRLF9e17GtSl",
        "outputId": "ffaf8949-8567-43d3-83bd-6809450720cd"
      },
      "outputs": [],
      "source": [
        "model = PPO(\"MultiInputPolicy\", env, verbose = 1, tensorboard_log=logdir)\n",
        "epoch = 1\n",
        "N_EPOCHS = 20\n",
        "\n",
        "for i in range(epoch, N_EPOCHS + 1):\n",
        "    model.learn(\n",
        "        total_timesteps=TIMESTEPS_PER_EPOCH,\n",
        "        reset_num_timesteps=False,\n",
        "        tb_log_name=f\"{model_name}\"\n",
        "    )\n",
        "    model.save(f\"{models_dir}/{TIMESTEPS_PER_EPOCH * i}\")\n",
        "    eval_model(env, model, figs_dir, TIMESTEPS_PER_EPOCH * i)\n",
        "    epoch += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oo4LTUy97rHj"
      },
      "source": [
        "## Evaluating the Policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FphzwEQoPh6U"
      },
      "outputs": [],
      "source": [
        "## TODO: THESE CELLS NEED TO BE REPLACED WITH THE NEW TESTING CODE\n",
        "\n",
        "model_name = \"PPO\"\n",
        "model_idx = 500000\n",
        "\n",
        "Ts = numerical_integration_parameters[\"Ts\"]\n",
        "path_for_saving_figures = f\"models/{model_name}/eval\"\n",
        "ensure_dir(path_for_saving_figures)\n",
        "\n",
        "# Model Definition\n",
        "#model = PPO(\"MultiInputPolicy\", env, verbose = 1)\n",
        "\n",
        "# Load Model\n",
        "#model_path = f'models/{model_name}/{str(model_idx)}.zip'\n",
        "#print(\"Loading saved model ..\")\n",
        "#model = PPO.load(model_path, env = env)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "itWlhkKNPh6U"
      },
      "source": [
        "### Test Reward Wrapper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3FQYZDxzPh6U"
      },
      "outputs": [],
      "source": [
        "## TODO: THESE CELLS NEED TO BE REPLACED WITH THE NEW TESTING CODE\n",
        "\n",
        "class TestRewardWrapper(gym.Wrapper):\n",
        "    def __init__(self, env):\n",
        "        super(TestRewardWrapper, self).__init__(env)\n",
        "\n",
        "    def step(self, action):\n",
        "        observation, reward, terminated, truncated, info = self.env.step(action)\n",
        "\n",
        "        # Create your Custom Test reward here\n",
        "        distance_to_line = abs(observation[\"distance_to_closest_point\"])\n",
        "        # Reward = 1 only if the car is within 1 meters of the line\n",
        "        if distance_to_line < 1:\n",
        "            reward = 1.0\n",
        "        else:\n",
        "            reward = 0\n",
        "\n",
        "        return observation, reward, terminated, truncated, info\n",
        "\n",
        "env = TestRewardWrapper(env)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QblIn1Thbr_Y"
      },
      "source": [
        "### Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "43Ps_SrjPh6U",
        "outputId": "b20e26c4-6017-4103-dd7d-a5a174506366"
      },
      "outputs": [],
      "source": [
        "## TODO: THESE CELLS NEED TO BE REPLACED WITH THE NEW TESTING CODE\n",
        "\n",
        "print(\"Evaluating model .. \")\n",
        "trajectory, rewards = evaluate_policy(\n",
        "    env, model, max_steps = 5000, return_rewards = True)\n",
        "\n",
        "total_reward = sum(rewards)\n",
        "avg_reward = total_reward/len(rewards)\n",
        "\n",
        "print(f\"Evaluation Complete. Total Reward for Episode: {total_reward:.0f}, Avg Reward/Step: {avg_reward:.2f}\")\n",
        "fig = plot_rewards(rewards)\n",
        "fig.savefig(f\"{path_for_saving_figures}/test_rewards.png\")\n",
        "\n",
        "print(\"Plotting and animating trajectory .. \")\n",
        "ani = plot_and_animate_trajectory(\n",
        "    env, trajectory, Ts, path_for_saving_figures\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 633
        },
        "id": "l-fErufY_Ntf",
        "outputId": "ec7a7985-4759-46ae-fd12-ea2afdde823d"
      },
      "outputs": [],
      "source": [
        "from IPython.display import HTML\n",
        "HTML(ani.to_jshtml())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Policy with Internal Lane Keeping Enabled"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a fresh env with ONLY internal lane keeping enabled\n",
        "internal_policy_config = {\n",
        "    \"enable_lane_keep\": True,\n",
        "    \"enable_cruise_control\": False,\n",
        "}\n",
        "\n",
        "env_lk = gym.make(\n",
        "    \"ai4rgym/autonomous_driving_env\",\n",
        "    render_mode=None,\n",
        "    bicycle_model_parameters=bicycle_model_parameters,\n",
        "    road_elements_list=road_elements_list,\n",
        "    numerical_integration_parameters=numerical_integration_parameters,\n",
        "    termination_parameters=termination_parameters,\n",
        "    initial_state_bounds=initial_state_bounds,\n",
        "    observation_parameters=observation_parameters,\n",
        "    internal_policy_config=internal_policy_config,\n",
        ")\n",
        "\n",
        "# Match earlier setup\n",
        "Ts_sim = 0.05\n",
        "integration_method = \"rk4\"\n",
        "env_lk.unwrapped.set_integration_method(integration_method)\n",
        "env_lk.unwrapped.set_integration_Ts(Ts_sim)\n",
        "env_lk.unwrapped.set_road_condition(road_condition=\"wet\")\n",
        "\n",
        "# Action scaling and reward shaping wrapper (same config as before)\n",
        "env_lk = gym.wrappers.RescaleAction(env_lk, min_action=-1, max_action=1)\n",
        "reward_cfg = {\"k_deviation\": 100.0, \"k_speed\": 1.0}\n",
        "env_lk = CustomRewardWrapper(env_lk, cfg=reward_cfg)\n",
        "\n",
        "# Rename model/run for this experiment\n",
        "model_name_lk = \"PPO_LK\"\n",
        "models_dir_lk = f\"models/{model_name_lk}\"\n",
        "figs_dir_lk = f\"{models_dir_lk}/figs\"\n",
        "ensure_dirs([models_dir_lk, figs_dir_lk])\n",
        "\n",
        "# Build model and train using the same epoch/timestep structure\n",
        "model_lk = PPO(\"MultiInputPolicy\", env_lk, verbose=1, tensorboard_log=logdir)\n",
        "\n",
        "start_epoch = 1\n",
        "N_EPOCHS_LK = 10  # follow previous pattern; adjust if desired\n",
        "\n",
        "for i in range(start_epoch, N_EPOCHS_LK + 1):\n",
        "    model_lk.learn(\n",
        "        total_timesteps=TIMESTEPS_PER_EPOCH,\n",
        "        reset_num_timesteps=False,\n",
        "        tb_log_name=f\"{model_name_lk}\",\n",
        "    )\n",
        "    step_count = TIMESTEPS_PER_EPOCH * i\n",
        "    model_lk.save(f\"{models_dir_lk}/{step_count}\")\n",
        "    eval_model(env_lk, model_lk, figs_dir_lk, step_count)\n",
        "    print(f\"[LK] Finished epoch {i}/{N_EPOCHS_LK} | saved at step {step_count}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Policy with Internal Cruise Control Enabled"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a fresh env with ONLY internal cruise control enabled\n",
        "internal_policy_config_cc = {\n",
        "    \"enable_lane_keep\": False,\n",
        "    \"enable_cruise_control\": True,\n",
        "    \"cruise_use_recommended_speed\": True,  # Use recommended speed for cruise control\n",
        "}\n",
        "\n",
        "env_cc = gym.make(\n",
        "    \"ai4rgym/autonomous_driving_env\",\n",
        "    render_mode=None,\n",
        "    bicycle_model_parameters=bicycle_model_parameters,\n",
        "    road_elements_list=road_elements_list,\n",
        "    numerical_integration_parameters=numerical_integration_parameters,\n",
        "    termination_parameters=termination_parameters,\n",
        "    initial_state_bounds=initial_state_bounds,\n",
        "    observation_parameters=observation_parameters,\n",
        "    internal_policy_config=internal_policy_config_cc,\n",
        ")\n",
        "\n",
        "# Match earlier setup\n",
        "Ts_sim = 0.05\n",
        "integration_method = \"rk4\"\n",
        "env_cc.unwrapped.set_integration_method(integration_method)\n",
        "env_cc.unwrapped.set_integration_Ts(Ts_sim)\n",
        "env_cc.unwrapped.set_road_condition(road_condition=\"wet\")\n",
        "\n",
        "env_cc = gym.wrappers.RescaleAction(env_cc, min_action=-1, max_action=1)\n",
        "reward_cfg = {\"k_deviation\": 100.0, \"k_speed\": 1.0}\n",
        "env_cc = CustomRewardWrapper(env_cc, cfg=reward_cfg)\n",
        "\n",
        "# Rename model/run for this experiment\n",
        "model_name_cc = \"PPO_CC\"\n",
        "models_dir_cc = f\"models/{model_name_cc}\"\n",
        "figs_dir_cc = f\"{models_dir_cc}/figs\"\n",
        "ensure_dirs([models_dir_cc, figs_dir_cc])\n",
        "\n",
        "# Build model and train using the same epoch/timestep structure\n",
        "model_cc = PPO(\"MultiInputPolicy\", env_cc, verbose=1, tensorboard_log=logdir)\n",
        "\n",
        "start_epoch = 1\n",
        "N_EPOCHS_CC = 10  # follow previous pattern; adjust if desired\n",
        "\n",
        "for i in range(start_epoch, N_EPOCHS_CC + 1):\n",
        "    model_cc.learn(\n",
        "        total_timesteps=TIMESTEPS_PER_EPOCH,\n",
        "        reset_num_timesteps=False,\n",
        "        tb_log_name=f\"{model_name_cc}\",\n",
        "    )\n",
        "    step_count = TIMESTEPS_PER_EPOCH * i\n",
        "    model_cc.save(f\"{models_dir_cc}/{step_count}\")\n",
        "    eval_model(env_cc, model_cc, figs_dir_cc, step_count)\n",
        "    print(f\"[CC] Finished epoch {i}/{N_EPOCHS_CC} | saved at step {step_count}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "RL",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
