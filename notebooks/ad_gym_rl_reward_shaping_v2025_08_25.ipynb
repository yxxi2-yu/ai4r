{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hKkJqp1Tlx8Y"
      },
      "source": [
        "# Autonomous Driving Gym: Reward Shaping & Domain Randomization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rM8Ygkkkl-_6"
      },
      "source": [
        "## Description\n",
        "\n",
        "\n",
        "This notebook provides a comprehensive guide to **Reward Shaping** and **Domain Randomization** within the Autonomous Driving Gym environment. It aims to support the development of an RL policy that enables an autonomous vehicle to maintain a recommended speed (cruise control) and follow the lane center (lane keeping).\n",
        "\n",
        "The notebook mainly covers the following key activities:\n",
        "\n",
        "1.  **Reward Shaping**:\n",
        "    *   Demonstrates the behavior of internal lane keeping and cruise control policies.\n",
        "    *   Explains and implements reward shaping specifically for lane keeping, where the agent controls steering while cruise control is handled internally.\n",
        "    *   Explains and implements reward shaping for cruise control, where the agent controls acceleration/braking while lane keeping is handled internally.\n",
        "    *   Introduces a combined reward wrapper that incorporates both lane keeping and cruise control objectives, allowing the agent to control both steering and acceleration/braking.\n",
        "2.  **Domain Randomization**:\n",
        "    *   Introduces the concept of domain randomization to improve the generalization of the trained RL policy.\n",
        "    *   Provides a `DomainRandomizationWrapper` to generate random road geometries during training.\n",
        "    *   Includes examples of different randomization parameters to create varying road complexities.\n",
        "    *   Sets up separate environments for training (with randomization) and evaluation (with a fixed, potentially difficult road).\n",
        "\n",
        "**RL Policy Training and Evaluation**:\n",
        "\n",
        "-   Demonstrates how to train a PPO agent using the shaped rewards and the domain-randomized environment.\n",
        "-   Provides helper functions to evaluate the trained policy on both training and evaluation environments.\n",
        "-   Includes visualization tools to plot trajectories and time series results.\n",
        "-   Introduces performance metrics to quantitatively assess the policy's performance based on distance to the lane center and speed.\n",
        "\n",
        "This is a long notebook, so please utilize the table of contents on the left for navigation (â‰¡). The \"Editing guidance\" section highlights the parts of the notebook that are most critical for you to modify and experiment with."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HV9nMDUFnXcI"
      },
      "source": [
        "## **Editing guidance**\n",
        "\n",
        "The frequently asked question that this \"editing guidance\" is answering is:\n",
        "\n",
        "**\"Which parts of the notebook should I change? Because sometimes it feels like there are 10-20 critical lines of code to changes out of >1000 lines of code!\"**\n",
        "\n",
        "We fully appreciate and agreed with the sentiment of this FAQ! Hence, the following is the list of sections within this notebook that are critical for you to edit so that you gain the intended experiece with PID synthesis and evaluation.\n",
        "\n",
        "**Note:** working through the whole notebook is still relevant for your understanding (even though you might not need to edit the parts not listed below)!\n",
        "\n",
        "**Note:** The (link to section) hyperlink may not work outside of google colab.\n",
        "\n",
        "---\n",
        "\n",
        "Here are the key sections where you can experiment and make changes:\n",
        "\n",
        "*   **Reward Shaping (Part 1)**:\n",
        "    *   **Lane Keeping Reward Shaping**: Decide which observations to use [here](#scrollTo=Observation_Parameters). Experiment with the `LaneKeepRewardWrapper` to shape the reward for lane keeping. You can modify the `k_distance` parameter and add other reward terms based on observations relevant to staying in the lane center. ([link to section](#scrollTo=Custom_Reward_Wrapper_Focusing_on_Lane_Keeping))\n",
        "    *   **Cruise Control Reward Shaping**: Decide which observations to use again. Modify the `CruiseControlRewardWrapper` to shape the reward for cruise control. You can adjust the `target_speed_mps`, `use_recommended`, `k_speed`, and introduce penalties for deviations from the target speed or recommended speed. ([link to section](#scrollTo=Reward_Wrapper_Focusing_on_Cruise_Control))\n",
        "    *   **Combined Reward Wrapper**: Experiment with the `CombinedRewardWrapper` by adjusting the weights (`k_distance`, `k_speed`) of the lane keeping and cruise control rewards, and by adding new reward components that consider the main as well as secondary objectives. ([link to section](#scrollTo=Combined_Reward_Wrapper))\n",
        "\n",
        "*   **Domain Randomization (Part 2)**:\n",
        "    *   **Road Randomization Parameters**: Modify the `road_randomization_params` dictionary to control the generation of random road geometries. Experiment with different ranges for the number of elements, straight lengths, curvatures, and angles to create various training environments. You can also experiment further with custom rewards here since domain randomization usually makes the models more robust. ([link to section](#scrollTo=Domain_Randomization_Wrapper_))\n",
        "    *   **Model Type**: You can experiment with different RL model types from `stable_baselines3` (e.g., `SAC`, `DDPG`, etc.) instead of PPO for training the policy. Alternatively, you can also update / configure the underlying neural network architectures of these models. Note that on-policy algorithms such as PPO are generally faster to train. ([link to section](#scrollTo=Model_Definition_and_Training_))\n",
        "\n",
        "*   **Performance Metrics**:\n",
        "    *   Modify the `compute_performance_metrics_from_time_series` function to define and calculate custom performance metrics that are relevant to your goals (e.g., smoothness of steering, efficiency of acceleration/braking, time spent off-road). ([link to section](#scrollTo=Define_a_performance_Metrics_Function_per_simulation_time_series_))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_b2dNRBjADs"
      },
      "source": [
        "# Dependencies (install and clone)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kcdeMTmr1l-S"
      },
      "outputs": [],
      "source": [
        "!pip install -q gymnasium\n",
        "!pip install -q stable_baselines3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4qASuf1o1IFk"
      },
      "outputs": [],
      "source": [
        "%cd /content\n",
        "%rm -rf ai4r-gym\n",
        "!git clone https://gitlab.unimelb.edu.au/ai4r/ai4r-gym.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vioqx1Rb1_PY"
      },
      "outputs": [],
      "source": [
        "%cd /content/ai4r-gym\n",
        "%pwd\n",
        "!git status"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Duy7Gv6BrG89"
      },
      "source": [
        "## Library Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4DcA6fIPrG89"
      },
      "outputs": [],
      "source": [
        "import ai4rgym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import gymnasium as gym\n",
        "from stable_baselines3 import PPO, SAC, DDPG\n",
        "from utils import ensure_dir, ensure_dirs, eval_model, evaluate_policy\n",
        "from utils import plot_rewards, plot_and_animate_trajectory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DN0lDy2UrG89"
      },
      "source": [
        "## Environment Settings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mMIt1UyZrG89"
      },
      "source": [
        "### Specify the Vehicle Parameters\n",
        "\n",
        "Dictionary with car specifications,\n",
        "in the form of a dynamic bicycle model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cg8HatPBrG89"
      },
      "outputs": [],
      "source": [
        "# SPECIFY THE VEHCILE PARAMETERS\n",
        "bicycle_model_parameters = {\n",
        "    \"Lf\" : 0.55*2.875,\n",
        "    \"Lr\" : 0.45*2.875,\n",
        "    \"m\"  : 2000.0,\n",
        "    \"Iz\" : (1.0/12.0) * 2000.0 * (4.692**2+1.850**2),\n",
        "    \"Cm\" : (1.0/100.0) * (1.0 * 400.0 * 9.0) / 0.2286,\n",
        "    \"Cd\" : 0.5 * 0.24 * 2.2204 * 1.202,\n",
        "    \"delta_offset\" : 0 * np.pi/180,\n",
        "    \"delta_request_max\" : 45 * np.pi/180,\n",
        "    \"Ddelta_lower_limit\" : -45 * np.pi/180,\n",
        "    \"Ddelta_upper_limit\" :  45 * np.pi/180,\n",
        "    \"v_transition_min\" : 500.0 / 3.6,\n",
        "    \"v_transition_max\" : 600.0 / 3.6,\n",
        "    \"body_len_f\" : (0.55*2.875) * 1.5,\n",
        "    \"body_len_r\" : (0.45*2.875) * 1.5,\n",
        "    \"body_width\" : 2.50,\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zigHDWNirG89"
      },
      "source": [
        "### Specify the Road\n",
        "\n",
        "\n",
        "Specified as a list of dictionaries, where each\n",
        "element in the list specifies a segment of the road.\n",
        "Example segment dictionaries:\n",
        "```Python\n",
        "{\"type\":\"straight\", \"length\":3.0}\n",
        "{\"type\":\"curved\", \"curvature\":1/50.0, \"angle_in_degrees\":45.0}\n",
        "{\"type\":\"curved\", \"curvature\":1/50.0, \"length\":30.0}\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I8rIJglurG89"
      },
      "outputs": [],
      "source": [
        "road_elements_list = [\n",
        "    {\"type\":\"straight\", \"length\":100.0, \"v_max_kph\":60.0},\n",
        "    {\"type\":\"curved\", \"curvature\":1/800.0, \"angle_in_degrees\":15.0, \"v_max_kph\":50.0},\n",
        "    {\"type\":\"straight\", \"length\":100.0, \"v_max_kph\":60.0},\n",
        "    {\"type\":\"curved\", \"curvature\":-1/400.0, \"angle_in_degrees\":30.0, \"v_max_kph\":40.0},\n",
        "    {\"type\":\"straight\", \"length\":100.0, \"v_max_kph\":60.0},\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUtosZXQrG89"
      },
      "source": [
        "### Specify the Numerical Integration Details\n",
        "\n",
        "The options available for the numerical\n",
        "integration method are:\n",
        "`[\"euler\", \"huen\", \"midpoint\", \"rk4\", \"rk45\"]`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_PKv4q_4rG89"
      },
      "outputs": [],
      "source": [
        "numerical_integration_parameters = {\n",
        "    \"method\" : \"rk4\",\n",
        "    \"Ts\" : 0.05,\n",
        "    \"num_steps_per_Ts\" : 1,\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YjxmZnjjrG89"
      },
      "source": [
        "###  Specify the Initial State Distribution\n",
        "\n",
        "The initial state is sampled from a uniform\n",
        "distribution between the minimum and maximum\n",
        "(i.e., between lower and upper bounds)\n",
        "> Note: a factor of (1/3.6) converts from units of [km/h] to [m/s]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kE1-ZQbbrG8-"
      },
      "outputs": [],
      "source": [
        "py_init_min = -1.0\n",
        "py_init_max =  1.0\n",
        "\n",
        "v_init_min_in_kmh = 55.0\n",
        "v_init_max_in_kmh = 65.0\n",
        "\n",
        "py_init_min = -1.0\n",
        "py_init_max =  1.0\n",
        "\n",
        "v_init_min_in_kmh = 55.0\n",
        "v_init_max_in_kmh = 65.0\n",
        "\n",
        "initial_state_bounds = {\n",
        "    \"px_init_min\" : 0.0,\n",
        "    \"px_init_max\" : 0.0,\n",
        "    \"py_init_min\" : py_init_min,\n",
        "    \"py_init_max\" : py_init_max,\n",
        "    \"theta_init_min\" : 0.0,\n",
        "    \"theta_init_max\" : 0.0,\n",
        "    \"vx_init_min\" : v_init_min_in_kmh * (1.0/3.6),\n",
        "    \"vx_init_max\" : v_init_max_in_kmh * (1.0/3.6),\n",
        "    \"vy_init_min\" : 0.0,\n",
        "    \"vy_init_max\" : 0.0,\n",
        "    \"omega_init_min\" : 0.0,\n",
        "    \"omega_init_max\" : 0.0,\n",
        "    \"delta_init_min\" : 0.0,\n",
        "    \"delta_init_max\" : 0.0,\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-GR5HrrrG8-"
      },
      "source": [
        "### Specify the Observation Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GjQ8xX6OrG8-"
      },
      "outputs": [],
      "source": [
        "# SPECIFY THE OBSERVATION PARAMETERS\n",
        "observation_parameters = {\n",
        "    \"should_include_ground_truth_px\"                       :  \"info\",\n",
        "    \"should_include_ground_truth_py\"                       :  \"info\",\n",
        "    \"should_include_ground_truth_theta\"                    :  \"info\",\n",
        "    \"should_include_ground_truth_vx\"                       :  \"info\",\n",
        "    \"should_include_ground_truth_vy\"                       :  \"info\",\n",
        "    \"should_include_ground_truth_omega\"                    :  \"info\",\n",
        "    \"should_include_ground_truth_delta\"                    :  \"info\",\n",
        "    \"should_include_road_progress_at_closest_point\"        :  \"info\",\n",
        "    \"should_include_vx_sensor\"                             :  \"obs\",\n",
        "    \"should_include_distance_to_closest_point\"             :  \"obs\",\n",
        "    \"should_include_heading_angle_relative_to_line\"        :  \"obs\",\n",
        "    \"should_include_heading_angular_rate_gyro\"             :  \"info\",\n",
        "    \"should_include_closest_point_coords_in_body_frame\"    :  \"info\",\n",
        "    \"should_include_look_ahead_line_coords_in_body_frame\"  :  \"info\",\n",
        "    \"should_include_road_curvature_at_closest_point\"       :  \"obs\",\n",
        "    \"should_include_look_ahead_road_curvatures\"            :  \"info\",\n",
        "    \"should_include_recommended_speed\"                     :  \"obs\",\n",
        "\n",
        "    \"scaling_for_ground_truth_px\"                       :  1.0,\n",
        "    \"scaling_for_ground_truth_py\"                       :  1.0,\n",
        "    \"scaling_for_ground_truth_theta\"                    :  1.0,\n",
        "    \"scaling_for_ground_truth_vx\"                       :  1.0,\n",
        "    \"scaling_for_ground_truth_vy\"                       :  1.0,\n",
        "    \"scaling_for_ground_truth_omega\"                    :  1.0,\n",
        "    \"scaling_for_ground_truth_delta\"                    :  1.0,\n",
        "    \"scaling_for_road_progress_at_closest_point\"        :  1.0,\n",
        "    \"scaling_for_vx_sensor\"                             :  1.0,\n",
        "    \"scaling_for_distance_to_closest_point\"             :  1.0,\n",
        "    \"scaling_for_heading_angle_relative_to_line\"        :  1.0,\n",
        "    \"scaling_for_heading_angular_rate_gyro\"             :  1.0,\n",
        "    \"scaling_for_closest_point_coords_in_body_frame\"    :  1.0,\n",
        "    \"scaling_for_look_ahead_line_coords_in_body_frame\"  :  1.0,\n",
        "    \"scaling_for_road_curvature_at_closest_point\"       :  1.0,\n",
        "    \"scaling_for_look_ahead_road_curvatures\"            :  1.0,\n",
        "\n",
        "    \"vx_sensor_bias\"    : 0.0,\n",
        "    \"vx_sensor_stddev\"  : 0.1,\n",
        "\n",
        "    \"distance_to_closest_point_bias\"    :  0.0,\n",
        "    \"distance_to_closest_point_stddev\"  :  0.01,\n",
        "\n",
        "    \"heading_angle_relative_to_line_bias\"    :  0.0,\n",
        "    \"heading_angle_relative_to_line_stddev\"  :  0.01,\n",
        "\n",
        "    \"heading_angular_rate_gyro_bias\"    :  0.0,\n",
        "    \"heading_angular_rate_gyro_stddev\"  :  0.01,\n",
        "\n",
        "    \"closest_point_coords_in_body_frame_bias\"    :  0.0,\n",
        "    \"closest_point_coords_in_body_frame_stddev\"  :  0.0,\n",
        "\n",
        "    \"look_ahead_line_coords_in_body_frame_bias\"    :  0.0,\n",
        "    \"look_ahead_line_coords_in_body_frame_stddev\"  :  0.0,\n",
        "\n",
        "    \"road_curvature_at_closest_point_bias\"    :  0.0,\n",
        "    \"road_curvature_at_closest_point_stddev\"  :  0.0,\n",
        "\n",
        "    \"look_ahead_road_curvatures_bias\"    :  0.0,\n",
        "    \"look_ahead_road_curvatures_stddev\"  :  0.0,\n",
        "\n",
        "    \"look_ahead_line_coords_in_body_frame_distance\"    :  100.0,\n",
        "    \"look_ahead_line_coords_in_body_frame_num_points\"  :  10,\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GC8QQtULrG8-"
      },
      "source": [
        "### Specify Termination Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4MQjwzv3rG8-"
      },
      "outputs": [],
      "source": [
        "termination_parameters = {\n",
        "    \"speed_lower_bound\"  :  0.0,\n",
        "    \"speed_upper_bound\"  :  (200.0/3.6),\n",
        "    \"distance_to_closest_point_upper_bound\"  :  20.0,\n",
        "    \"reward_speed_lower_bound\"  :  0.0,\n",
        "    \"reward_speed_upper_bound\"  :  0.0,\n",
        "    \"reward_distance_to_closest_point_upper_bound\"  :  0.0,\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYUWFl6TrG8-"
      },
      "source": [
        "## Part 1: Reward Shaping"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIhetKp6rG8-"
      },
      "source": [
        "\n",
        "### 1) Demo: Internal Lane Keeping and Cruise Control\n",
        "\n",
        "We enable both internal controllers and set `action_interface=\"auto\"`, which collapses the external action to a noâ€‘op. A neutral policy will send zeros; the internals will drive.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dRGR86zyrG8-"
      },
      "outputs": [],
      "source": [
        "# Build environment (reuse dictionaries defined above)\n",
        "from evaluation.evaluation_for_autonomous_driving import simulate_policy, plot_results_from_time_series_dict, plot_road_from_list_of_road_elements\n",
        "\n",
        "# Helper policy that returns a neutral action\n",
        "class NoOpPolicy:\n",
        "    def compute_action(self, observation, info_dict, terminated, truncated):\n",
        "        # Action shape is determined by the env's action_interface\n",
        "        # For \"none\": shape is (1,) and value is ignored\n",
        "        import numpy as np\n",
        "        low = env.action_space.low\n",
        "        high = env.action_space.high\n",
        "        return ((low + high) * 0.0).astype('float32')\n",
        "\n",
        "# Internal policies: both enabled\n",
        "internal_policy_config_both = {\n",
        "    \"enable_lane_keep\": True,\n",
        "    \"enable_cruise_control\": True,\n",
        "    \"action_interface\": \"none\",  # derives to \"none\" when both are enabled\n",
        "}\n",
        "\n",
        "env = gym.make(\n",
        "    \"ai4rgym/autonomous_driving_env\",\n",
        "    render_mode=None,\n",
        "    bicycle_model_parameters=bicycle_model_parameters,\n",
        "    road_elements_list=road_elements_list,\n",
        "    numerical_integration_parameters=numerical_integration_parameters,\n",
        "    termination_parameters=termination_parameters,\n",
        "    initial_state_bounds=initial_state_bounds,\n",
        "    observation_parameters=observation_parameters,\n",
        "    internal_policy_config=internal_policy_config_both,\n",
        ")\n",
        "\n",
        "# Optional surface condition\n",
        "env.unwrapped.set_road_condition(road_condition=\"dry\")\n",
        "\n",
        "# Run simulation with neutral action policy (let internals drive)\n",
        "N_sim = 1500\n",
        "results = simulate_policy(env, N_sim, NoOpPolicy(), seed=42, should_save_look_ahead_results=False, should_save_observations=True, verbose=1)\n",
        "\n",
        "save_dir = \"models/internal_policies\"\n",
        "ensure_dir(save_dir)\n",
        "\n",
        "# Quick trajectory/time-series plots\n",
        "plot_results_from_time_series_dict(env, results, save_dir, file_name_suffix=\"internal_policies\", should_plot_reward=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTHkU7z4rG8-"
      },
      "source": [
        "\n",
        "### 2) Lane keeping reward shaping (cruise control internal)\n",
        "\n",
        "We enable internal cruise control so the longitudinal speed is handled by the environment. We define a `LaneKeepRewardWrapper` that rewards staying near the lane center. A simple steering policy is provided as a working baseline; students are encouraged to iterate on the reward.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sID6VG7DrG8-"
      },
      "source": [
        "\n",
        "#### Observation Parameters\n",
        "\n",
        "We restrict observations to the signals relevant for lane keeping. You can edit this dictionary.\n",
        "- `distance_to_closest_point` (obs)\n",
        "- `heading_angle_relative_to_line` (obs)\n",
        "\n",
        "We explicitly disable unrelated default observations (e.g., `vx_sensor`, `recommended_speed`) to keep the input minimal.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h55eYGHTrG8-"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Minimal observation dictionary for lane keeping\n",
        "observation_parameters_lk = {\n",
        "    \"should_include_ground_truth_px\"                       :  \"info\",\n",
        "    \"should_include_ground_truth_py\"                       :  \"info\",\n",
        "    \"should_include_ground_truth_theta\"                    :  \"info\",\n",
        "    \"should_include_ground_truth_vx\"                       :  \"info\",\n",
        "    \"should_include_ground_truth_vy\"                       :  \"info\",\n",
        "    \"should_include_ground_truth_omega\"                    :  \"info\",\n",
        "    \"should_include_ground_truth_delta\"                    :  \"info\",\n",
        "    \"should_include_road_progress_at_closest_point\"        :  \"info\",\n",
        "    \"should_include_vx_sensor\"                             :  \"info\",\n",
        "    \"should_include_distance_to_closest_point\"             :  \"obs\",\n",
        "    \"should_include_heading_angle_relative_to_line\"        :  \"obs\",\n",
        "    \"should_include_heading_angular_rate_gyro\"             :  \"info\",\n",
        "    \"should_include_closest_point_coords_in_body_frame\"    :  \"info\",\n",
        "    \"should_include_look_ahead_line_coords_in_body_frame\"  :  \"info\",\n",
        "    \"should_include_road_curvature_at_closest_point\"       :  \"info\",\n",
        "    \"should_include_look_ahead_road_curvatures\"            :  \"info\",\n",
        "\n",
        "    \"scaling_for_ground_truth_px\"                       :  1.0,\n",
        "    \"scaling_for_ground_truth_py\"                       :  1.0,\n",
        "    \"scaling_for_ground_truth_theta\"                    :  1.0,\n",
        "    \"scaling_for_ground_truth_vx\"                       :  1.0,\n",
        "    \"scaling_for_ground_truth_vy\"                       :  1.0,\n",
        "    \"scaling_for_ground_truth_omega\"                    :  1.0,\n",
        "    \"scaling_for_ground_truth_delta\"                    :  1.0,\n",
        "    \"scaling_for_road_progress_at_closest_point\"        :  1.0,\n",
        "    \"scaling_for_vx_sensor\"                             :  1.0,\n",
        "    \"scaling_for_distance_to_closest_point\"             :  1.0,\n",
        "    \"scaling_for_heading_angle_relative_to_line\"        :  1.0,\n",
        "    \"scaling_for_heading_angular_rate_gyro\"             :  1.0,\n",
        "    \"scaling_for_closest_point_coords_in_body_frame\"    :  1.0,\n",
        "    \"scaling_for_look_ahead_line_coords_in_body_frame\"  :  1.0,\n",
        "    \"scaling_for_road_curvature_at_closest_point\"       :  1.0,\n",
        "    \"scaling_for_look_ahead_road_curvatures\"            :  1.0,\n",
        "\n",
        "    \"vx_sensor_bias\"    : 0.0,\n",
        "    \"vx_sensor_stddev\"  : 0.1,\n",
        "\n",
        "    \"distance_to_closest_point_bias\"    :  0.0,\n",
        "    \"distance_to_closest_point_stddev\"  :  0.01,\n",
        "\n",
        "    \"heading_angle_relative_to_line_bias\"    :  0.0,\n",
        "    \"heading_angle_relative_to_line_stddev\"  :  0.01,\n",
        "\n",
        "    \"heading_angular_rate_gyro_bias\"    :  0.0,\n",
        "    \"heading_angular_rate_gyro_stddev\"  :  0.01,\n",
        "\n",
        "    \"closest_point_coords_in_body_frame_bias\"    :  0.0,\n",
        "    \"closest_point_coords_in_body_frame_stddev\"  :  0.0,\n",
        "\n",
        "    \"look_ahead_line_coords_in_body_frame_bias\"    :  0.0,\n",
        "    \"look_ahead_line_coords_in_body_frame_stddev\"  :  0.0,\n",
        "\n",
        "    \"road_curvature_at_closest_point_bias\"    :  0.0,\n",
        "    \"road_curvature_at_closest_point_stddev\"  :  0.0,\n",
        "\n",
        "    \"look_ahead_road_curvatures_bias\"    :  0.0,\n",
        "    \"look_ahead_road_curvatures_stddev\"  :  0.0,\n",
        "\n",
        "    \"look_ahead_line_coords_in_body_frame_distance\"    :  100.0,\n",
        "    \"look_ahead_line_coords_in_body_frame_num_points\"  :  10,\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xAOgWqJnrG8-"
      },
      "source": [
        "ðŸ’¡\n",
        "- **Think**: *What observations are most relevant for lane keeping?*\n",
        "- **Note**: It's always good to start simple and add additional observations as needed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I86Ky2h8rG8-"
      },
      "source": [
        "####  Custom Reward Wrapper Focusing on Lane Keeping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WzyQ669grG8-"
      },
      "outputs": [],
      "source": [
        "\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "\n",
        "class LaneKeepRewardWrapper(gym.Wrapper):\n",
        "    \"\"\"Reward based on distance to lane center (smaller is better).\"\"\"\n",
        "    def __init__(self, env, k_distance: float = 1.0):\n",
        "        super().__init__(env)\n",
        "        self.k_distance = float(k_distance)\n",
        "    def reset(self, **kwargs):\n",
        "        return super().reset(**kwargs)\n",
        "    def step(self, action):\n",
        "        obs, base_r, terminated, truncated, info = self.env.step(action)\n",
        "        # Strict access: these keys must exist in the env's ground-truth dict\n",
        "        gt = self.env.unwrapped.current_ground_truth\n",
        "        d = float(abs(gt[\"distance_to_closest_point\"]))\n",
        "        # Reward: larger when closer to center; tapers off with distance\n",
        "        r = self.k_distance * (1.0 / (1.0 + d))\n",
        "        return obs, r, terminated, truncated, info\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "umUShTbkrG8-"
      },
      "source": [
        "#### ðŸ’¡ Possible options to consider:\n",
        "\n",
        "- [ ] *pick 1-2 changes from below and try to implement*\n",
        "\n",
        "---\n",
        "\n",
        "- Very high reward for being near the lane center\n",
        "- Smaller reward for staying on the lane/road\n",
        "- Penalty for large steering actions\n",
        "- Quadratic penalty / rewards for stronger learning/reward signals\n",
        "- Penalty for going off-road\n",
        "- Penalty for large heading angle relative to lane direction\n",
        "\n",
        "\n",
        "> (including but not limited to, feel free to innovate!)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rk_m5F0XrG8-"
      },
      "source": [
        "#### Environment Setup for Lane Keeping (Cruise Control Internal)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x4LrFP2XrG8-"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Internal policy: cruise control on, lane-keeping off (agent controls steering)\n",
        "internal_policy_config_lk = {\n",
        "    \"enable_lane_keep\": False,\n",
        "    \"enable_cruise_control\": True,\n",
        "    \"action_interface\": \"steer_only\",\n",
        "    # Optionally tune cruise control target speed\n",
        "    \"cruise_use_recommended_speed\": True,\n",
        "    \"cruise_target_speed_mps\": 60.0 / 3.6,\n",
        "}\n",
        "\n",
        "# Build env and wrap with lane-keeping reward\n",
        "import gymnasium as gym\n",
        "from evaluation.evaluation_for_autonomous_driving import simulate_policy, plot_results_from_time_series_dict\n",
        "\n",
        "env_lk = gym.make(\n",
        "    \"ai4rgym/autonomous_driving_env\",\n",
        "    render_mode=None,\n",
        "    bicycle_model_parameters=bicycle_model_parameters,\n",
        "    road_elements_list=road_elements_list,\n",
        "    numerical_integration_parameters=numerical_integration_parameters,\n",
        "    termination_parameters=termination_parameters,\n",
        "    initial_state_bounds=initial_state_bounds,\n",
        "    observation_parameters=observation_parameters_lk,\n",
        "    internal_policy_config=internal_policy_config_lk,\n",
        ")\n",
        "\n",
        "env_lk = LaneKeepRewardWrapper(env_lk, k_distance=1.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ksr_bsNBrG8-"
      },
      "source": [
        "#### Training RL Policy for Lane Keeping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j4DUIlGdrG8-"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Train a small PPO agent to verify the shaped reward works\n",
        "from stable_baselines3 import PPO\n",
        "import gymnasium as gym\n",
        "\n",
        "# Rescale the 1-D steering action to [-1, 1] for PPO stability\n",
        "env_lk_train = gym.wrappers.RescaleAction(env_lk, min_action=-1, max_action=1)\n",
        "\n",
        "# Rename model/run for this experiment\n",
        "logdir = \"logs\"\n",
        "model_name_lk = \"PPO_LK\"\n",
        "models_dir_lk = f\"models/{model_name_lk}\"\n",
        "figs_dir_lk = f\"{models_dir_lk}/figs\"\n",
        "ensure_dirs([models_dir_lk, figs_dir_lk])\n",
        "\n",
        "TIMESTEPS_PER_EPOCH = 50000\n",
        "\n",
        "# Build model and train using the same epoch/timestep structure\n",
        "model_lk = PPO(\"MultiInputPolicy\", env_lk_train, verbose=1, tensorboard_log=logdir)\n",
        "\n",
        "start_epoch = 1\n",
        "N_EPOCHS_LK = 10  # follow previous pattern; adjust if desired\n",
        "\n",
        "for i in range(start_epoch, N_EPOCHS_LK + 1):\n",
        "    model_lk.learn(\n",
        "        total_timesteps=TIMESTEPS_PER_EPOCH,\n",
        "        reset_num_timesteps=False,\n",
        "        tb_log_name=f\"{model_name_lk}\",\n",
        "    )\n",
        "    step_count = TIMESTEPS_PER_EPOCH * i\n",
        "    model_lk.save(f\"{models_dir_lk}/{step_count}\")\n",
        "    eval_model(env_lk_train, model_lk, figs_dir_lk, step_count, plot_velocity=True)\n",
        "    print(f\"[LK] Finished epoch {i}/{N_EPOCHS_LK} | saved at step {step_count}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-KHUnWb3rG8-"
      },
      "source": [
        "ðŸ’¡ Look inside the `models/{model_name}/figs` directory for visualizations and insights related to the model's performance, such as - reward values, speed, trajectory, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HQlbjlLrG8-"
      },
      "source": [
        "\n",
        "### 3) Cruise control reward shaping (lane keeping internal)\n",
        "\n",
        "We enable internal lane keeping so the lateral control is handled by the environment. We define a `CruiseControlRewardWrapper` that rewards staying near a target speed (or the recommended speed). A simple throttle policy is provided as a baseline.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNxJXBu9rG8-"
      },
      "source": [
        "\n",
        "#### Observation Parameters\n",
        "\n",
        "We restrict observations to the signals relevant for cruise control shaping:\n",
        "- `vx_sensor` (obs): longitudinal speed measurement used by the agent/policy.\n",
        "- `recommended_speed` (info): ground-truth recommended speed from the road geometry.\n",
        "\n",
        "We keep other recommendations in `info` (not observations) to keep the\n",
        "learning target simple; the reward wrapper can still access ground-truth values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2VbBDZY5rG8_"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Minimal observation dictionary for lane keeping\n",
        "observation_parameters_cc = {\n",
        "    \"should_include_ground_truth_px\"                       :  \"info\",\n",
        "    \"should_include_ground_truth_py\"                       :  \"info\",\n",
        "    \"should_include_ground_truth_theta\"                    :  \"info\",\n",
        "    \"should_include_ground_truth_vx\"                       :  \"info\",\n",
        "    \"should_include_ground_truth_vy\"                       :  \"info\",\n",
        "    \"should_include_ground_truth_omega\"                    :  \"info\",\n",
        "    \"should_include_ground_truth_delta\"                    :  \"info\",\n",
        "    \"should_include_road_progress_at_closest_point\"        :  \"info\",\n",
        "    \"should_include_vx_sensor\"                             :  \"obs\",\n",
        "    \"should_include_distance_to_closest_point\"             :  \"info\",\n",
        "    \"should_include_heading_angle_relative_to_line\"        :  \"info\",\n",
        "    \"should_include_heading_angular_rate_gyro\"             :  \"info\",\n",
        "    \"should_include_closest_point_coords_in_body_frame\"    :  \"info\",\n",
        "    \"should_include_look_ahead_line_coords_in_body_frame\"  :  \"info\",\n",
        "    \"should_include_road_curvature_at_closest_point\"       :  \"info\",\n",
        "    \"should_include_look_ahead_road_curvatures\"            :  \"info\",\n",
        "    \"should_include_recommended_speed\"                     :  \"obs\",\n",
        "    \"should_include_speed_limit\"                           :  \"info\",\n",
        "\n",
        "    \"scaling_for_ground_truth_px\"                       :  1.0,\n",
        "    \"scaling_for_ground_truth_py\"                       :  1.0,\n",
        "    \"scaling_for_ground_truth_theta\"                    :  1.0,\n",
        "    \"scaling_for_ground_truth_vx\"                       :  1.0,\n",
        "    \"scaling_for_ground_truth_vy\"                       :  1.0,\n",
        "    \"scaling_for_ground_truth_omega\"                    :  1.0,\n",
        "    \"scaling_for_ground_truth_delta\"                    :  1.0,\n",
        "    \"scaling_for_road_progress_at_closest_point\"        :  1.0,\n",
        "    \"scaling_for_vx_sensor\"                             :  1.0,\n",
        "    \"scaling_for_distance_to_closest_point\"             :  1.0,\n",
        "    \"scaling_for_heading_angle_relative_to_line\"        :  1.0,\n",
        "    \"scaling_for_heading_angular_rate_gyro\"             :  1.0,\n",
        "    \"scaling_for_closest_point_coords_in_body_frame\"    :  1.0,\n",
        "    \"scaling_for_look_ahead_line_coords_in_body_frame\"  :  1.0,\n",
        "    \"scaling_for_road_curvature_at_closest_point\"       :  1.0,\n",
        "    \"scaling_for_look_ahead_road_curvatures\"            :  1.0,\n",
        "\n",
        "    \"vx_sensor_bias\"    : 0.0,\n",
        "    \"vx_sensor_stddev\"  : 0.1,\n",
        "\n",
        "    \"distance_to_closest_point_bias\"    :  0.0,\n",
        "    \"distance_to_closest_point_stddev\"  :  0.01,\n",
        "\n",
        "    \"heading_angle_relative_to_line_bias\"    :  0.0,\n",
        "    \"heading_angle_relative_to_line_stddev\"  :  0.01,\n",
        "\n",
        "    \"heading_angular_rate_gyro_bias\"    :  0.0,\n",
        "    \"heading_angular_rate_gyro_stddev\"  :  0.01,\n",
        "\n",
        "    \"closest_point_coords_in_body_frame_bias\"    :  0.0,\n",
        "    \"closest_point_coords_in_body_frame_stddev\"  :  0.0,\n",
        "\n",
        "    \"look_ahead_line_coords_in_body_frame_bias\"    :  0.0,\n",
        "    \"look_ahead_line_coords_in_body_frame_stddev\"  :  0.0,\n",
        "\n",
        "    \"road_curvature_at_closest_point_bias\"    :  0.0,\n",
        "    \"road_curvature_at_closest_point_stddev\"  :  0.0,\n",
        "\n",
        "    \"look_ahead_road_curvatures_bias\"    :  0.0,\n",
        "    \"look_ahead_road_curvatures_stddev\"  :  0.0,\n",
        "\n",
        "    \"look_ahead_line_coords_in_body_frame_distance\"    :  100.0,\n",
        "    \"look_ahead_line_coords_in_body_frame_num_points\"  :  10,\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rx8oHqp5rG8_"
      },
      "source": [
        "ðŸ’¡\n",
        "- **Think**: *What observations are most relevant for cruise control?*\n",
        "- **Note**: It's always good to start simple and add additional observations as needed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nrA25ptwrG8_"
      },
      "source": [
        "#### Reward Wrapper Focusing on Cruise Control"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uqi2fpPWrG8_"
      },
      "outputs": [],
      "source": [
        "\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "\n",
        "class CruiseControlRewardWrapper(gym.Wrapper):\n",
        "    \"\"\"Reward based on proximity to a target speed (m/s), gated by road heading.\"\"\"\n",
        "    def __init__(self, env, target_speed_mps: float = 60.0/3.6, use_recommended: bool = False, k_speed: float = 1.0):\n",
        "        super().__init__(env)\n",
        "        self.target_speed_mps = float(target_speed_mps)\n",
        "        self.use_recommended = bool(use_recommended)\n",
        "        self.k_speed = float(k_speed)\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        return super().reset(**kwargs)\n",
        "\n",
        "    def step(self, action):\n",
        "        obs, base_r, terminated, truncated, info = self.env.step(action)\n",
        "\n",
        "        # Ground-truth values from env\n",
        "        v = float(self.env.unwrapped.car.vx)\n",
        "        gt = self.env.unwrapped.current_ground_truth\n",
        "        v_rec = float(gt['recommended_speed_at_closest_point'])\n",
        "        v_tgt = v_rec if (self.use_recommended and v_rec > 0.0) else self.target_speed_mps\n",
        "\n",
        "        # Heading relative to road line (assumed radians)\n",
        "        theta = float(gt['heading_angle_relative_to_line'])\n",
        "        gate = max(0.0, np.cos(theta))  # 1 when aligned, 0 when â‰¥90Â° off\n",
        "\n",
        "        r_speed = self.k_speed * (1.0 / (1.0 + abs(v - v_tgt)))\n",
        "        r = gate * r_speed\n",
        "\n",
        "        # Debug info (optional, drop if too noisy)\n",
        "        info = dict(info, heading_rel_line=theta, heading_gate=gate, r_speed=r_speed)\n",
        "\n",
        "        return obs, r, terminated, truncated, info"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-saKGVPTrG8_"
      },
      "source": [
        "#### ðŸ’¡ Possible options to consider:\n",
        "\n",
        "- [ ] *pick 1-2 changes from below and try to implement*\n",
        "\n",
        "---\n",
        "\n",
        "- High reward for being near the recommended speed\n",
        "- Flat reward closer to the recommended speed to encourage practical driving\n",
        "- Smaller reward for going above the recommended speed\n",
        "- Large penalty for exceeding the speed limit\n",
        "- Penalty for going below a certain speed\n",
        "- Penalty for large acceleration actions\n",
        "- Quadratic penalty / rewards for stronger learning/reward signals\n",
        "\n",
        "> (including but not limited to, feel free to innovate!)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9mWRb8crG8_"
      },
      "source": [
        "#### Environment Setup for Cruise Control (Lane Keeping Internal)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5kHGxH58rG8_"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Internal policy: lane keeping on, cruise control off (agent controls drive)\n",
        "internal_policy_config_cc = {\n",
        "    \"enable_lane_keep\": True,\n",
        "    \"enable_cruise_control\": False,\n",
        "    \"action_interface\": \"drive_only\",\n",
        "}\n",
        "\n",
        "import gymnasium as gym\n",
        "from evaluation.evaluation_for_autonomous_driving import simulate_policy, plot_results_from_time_series_dict\n",
        "\n",
        "env_cc = gym.make(\n",
        "    \"ai4rgym/autonomous_driving_env\",\n",
        "    render_mode=None,\n",
        "    bicycle_model_parameters=bicycle_model_parameters,\n",
        "    road_elements_list=road_elements_list,\n",
        "    numerical_integration_parameters=numerical_integration_parameters,\n",
        "    termination_parameters=termination_parameters,\n",
        "    initial_state_bounds=initial_state_bounds,\n",
        "    observation_parameters=observation_parameters_cc,\n",
        "    internal_policy_config=internal_policy_config_cc,\n",
        ")\n",
        "\n",
        "env_cc = CruiseControlRewardWrapper(env_cc, target_speed_mps=60.0/3.6, use_recommended=True, k_speed=1.0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wV7FNJX2rG8_"
      },
      "source": [
        "#### Training RL Policy for Cruise Control"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3pj4QFe-rG8_"
      },
      "outputs": [],
      "source": [
        "# Train a small PPO agent to verify the shaped reward works\n",
        "from stable_baselines3 import PPO\n",
        "import gymnasium as gym\n",
        "\n",
        "# Rescale the 1-D drive action to [-1, 1] for PPO stability\n",
        "env_cc_train = gym.wrappers.RescaleAction(env_cc, min_action=-1, max_action=1)\n",
        "\n",
        "logdir = \"logs\"\n",
        "\n",
        "# Rename model/run for this experiment\n",
        "model_name_cc = \"PPO_CC\"\n",
        "models_dir_cc = f\"models/{model_name_cc}\"\n",
        "figs_dir_cc = f\"{models_dir_cc}/figs\"\n",
        "ensure_dirs([models_dir_cc, figs_dir_cc])\n",
        "\n",
        "TIMESTEPS_PER_EPOCH = 50000\n",
        "\n",
        "# Build model and train using the same epoch/timestep structure\n",
        "model_cc = PPO(\"MultiInputPolicy\", env_cc_train, verbose=1, tensorboard_log=logdir)\n",
        "\n",
        "start_epoch = 1\n",
        "N_EPOCHS_CC = 10  # follow previous pattern; adjust if desired\n",
        "\n",
        "for i in range(start_epoch, N_EPOCHS_CC + 1):\n",
        "    model_cc.learn(\n",
        "        total_timesteps=TIMESTEPS_PER_EPOCH,\n",
        "        reset_num_timesteps=False,\n",
        "        tb_log_name=f\"{model_name_cc}\",\n",
        "    )\n",
        "    step_count = TIMESTEPS_PER_EPOCH * i\n",
        "    model_cc.save(f\"{models_dir_cc}/{step_count}\")\n",
        "    eval_model(env_cc_train, model_cc, figs_dir_cc, step_count, plot_velocity=True)\n",
        "    print(f\"[CC] Finished epoch {i}/{N_EPOCHS_CC} | saved at step {step_count}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMSAtOJerG8_"
      },
      "source": [
        "\n",
        "### 4) Combine lane-keeping and cruise rewards\n",
        "\n",
        "We combine the rewards into a single `CombinedRewardWrapper`.  \n",
        "Additional observations can be enabled to support both tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-fpt8pRerG8_"
      },
      "outputs": [],
      "source": [
        "\n",
        "observation_parameters = {\n",
        "    \"should_include_ground_truth_px\"                       :  \"info\",\n",
        "    \"should_include_ground_truth_py\"                       :  \"info\",\n",
        "    \"should_include_ground_truth_theta\"                    :  \"info\",\n",
        "    \"should_include_ground_truth_vx\"                       :  \"info\",\n",
        "    \"should_include_ground_truth_vy\"                       :  \"info\",\n",
        "    \"should_include_ground_truth_omega\"                    :  \"info\",\n",
        "    \"should_include_ground_truth_delta\"                    :  \"info\",\n",
        "    \"should_include_road_progress_at_closest_point\"        :  \"info\",\n",
        "    \"should_include_vx_sensor\"                             :  \"obs\",\n",
        "    \"should_include_distance_to_closest_point\"             :  \"obs\",\n",
        "    \"should_include_heading_angle_relative_to_line\"        :  \"obs\",\n",
        "    \"should_include_heading_angular_rate_gyro\"             :  \"info\",\n",
        "    \"should_include_closest_point_coords_in_body_frame\"    :  \"info\",\n",
        "    \"should_include_look_ahead_line_coords_in_body_frame\"  :  \"info\",\n",
        "    \"should_include_road_curvature_at_closest_point\"       :  \"info\",\n",
        "    \"should_include_look_ahead_road_curvatures\"            :  \"info\",\n",
        "    \"should_include_recommended_speed\"                     :  \"obs\",\n",
        "    \"should_include_speed_limit\"                           :  \"info\",\n",
        "\n",
        "    \"scaling_for_ground_truth_px\"                       :  1.0,\n",
        "    \"scaling_for_ground_truth_py\"                       :  1.0,\n",
        "    \"scaling_for_ground_truth_theta\"                    :  1.0,\n",
        "    \"scaling_for_ground_truth_vx\"                       :  1.0,\n",
        "    \"scaling_for_ground_truth_vy\"                       :  1.0,\n",
        "    \"scaling_for_ground_truth_omega\"                    :  1.0,\n",
        "    \"scaling_for_ground_truth_delta\"                    :  1.0,\n",
        "    \"scaling_for_road_progress_at_closest_point\"        :  1.0,\n",
        "    \"scaling_for_vx_sensor\"                             :  1.0,\n",
        "    \"scaling_for_distance_to_closest_point\"             :  1.0,\n",
        "    \"scaling_for_heading_angle_relative_to_line\"        :  1.0,\n",
        "    \"scaling_for_heading_angular_rate_gyro\"             :  1.0,\n",
        "    \"scaling_for_closest_point_coords_in_body_frame\"    :  1.0,\n",
        "    \"scaling_for_look_ahead_line_coords_in_body_frame\"  :  1.0,\n",
        "    \"scaling_for_road_curvature_at_closest_point\"       :  1.0,\n",
        "    \"scaling_for_look_ahead_road_curvatures\"            :  1.0,\n",
        "\n",
        "    \"vx_sensor_bias\"    : 0.0,\n",
        "    \"vx_sensor_stddev\"  : 0.1,\n",
        "\n",
        "    \"distance_to_closest_point_bias\"    :  0.0,\n",
        "    \"distance_to_closest_point_stddev\"  :  0.01,\n",
        "\n",
        "    \"heading_angle_relative_to_line_bias\"    :  0.0,\n",
        "    \"heading_angle_relative_to_line_stddev\"  :  0.01,\n",
        "\n",
        "    \"heading_angular_rate_gyro_bias\"    :  0.0,\n",
        "    \"heading_angular_rate_gyro_stddev\"  :  0.01,\n",
        "\n",
        "    \"closest_point_coords_in_body_frame_bias\"    :  0.0,\n",
        "    \"closest_point_coords_in_body_frame_stddev\"  :  0.0,\n",
        "\n",
        "    \"look_ahead_line_coords_in_body_frame_bias\"    :  0.0,\n",
        "    \"look_ahead_line_coords_in_body_frame_stddev\"  :  0.0,\n",
        "\n",
        "    \"road_curvature_at_closest_point_bias\"    :  0.0,\n",
        "    \"road_curvature_at_closest_point_stddev\"  :  0.0,\n",
        "\n",
        "    \"look_ahead_road_curvatures_bias\"    :  0.0,\n",
        "    \"look_ahead_road_curvatures_stddev\"  :  0.0,\n",
        "\n",
        "    \"look_ahead_line_coords_in_body_frame_distance\"    :  100.0,\n",
        "    \"look_ahead_line_coords_in_body_frame_num_points\"  :  10,\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lGJZhWhrG8_"
      },
      "source": [
        "#### Combined Reward Wrapper"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** When combining multiple reward values, it's important that they are balanced (i.e. one reward term does not dominate / have very low values). You can use the k_terms to balance multiple rewards. Both distance and speed rewards are kept between 0-1 in this example and appear in similar scales. It's a good idea to keep reward values within gentle bounds (while also having sufficient resolution) to promote stability.   "
      ],
      "metadata": {
        "id": "f9HOFmonwfyi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rLHAg5qNrG8_"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "\n",
        "class CombinedRewardWrapper(gym.Wrapper):\n",
        "    \"\"\"\n",
        "    Reward combining lane keeping (distance to center) and cruise control\n",
        "    (target speed with heading gating).\n",
        "    \"\"\"\n",
        "    def __init__(self, env,\n",
        "                 k_distance: float = 1.0,\n",
        "                 k_speed: float = 1.0,\n",
        "                 target_speed_mps: float = 60.0/3.6,\n",
        "                 use_recommended: bool = False):\n",
        "        super().__init__(env)\n",
        "        self.k_distance = float(k_distance)\n",
        "        self.k_speed = float(k_speed)\n",
        "        self.target_speed_mps = float(target_speed_mps)\n",
        "        self.use_recommended = bool(use_recommended)\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        return super().reset(**kwargs)\n",
        "\n",
        "    def step(self, action):\n",
        "        obs, base_r, terminated, truncated, info = self.env.step(action)\n",
        "\n",
        "        # Ground-truth values\n",
        "        gt = self.env.unwrapped.current_ground_truth\n",
        "        v = float(self.env.unwrapped.car.vx)\n",
        "        d = float(abs(gt[\"distance_to_closest_point\"]))\n",
        "        v_rec = float(gt[\"recommended_speed_at_closest_point\"])\n",
        "        theta = float(gt[\"heading_angle_relative_to_line\"])\n",
        "\n",
        "        # Lane-keeping reward: closer to center â†’ larger reward\n",
        "        r_lane = self.k_distance * (1.0 / (1.0 + d))\n",
        "\n",
        "        # Cruise-control reward: closer to target speed (with heading gating)\n",
        "        v_tgt = v_rec if (self.use_recommended and v_rec > 0.0) else self.target_speed_mps\n",
        "        gate = max(0.0, np.cos(theta))  # downweight when poorly aligned\n",
        "        r_speed = self.k_speed * (1.0 / (1.0 + abs(v - v_tgt)))\n",
        "\n",
        "        # Final reward: sum of both\n",
        "        r = r_lane + gate * r_speed\n",
        "\n",
        "        # Add debug info\n",
        "        info = dict(info,\n",
        "                    lane_distance=d,\n",
        "                    r_lane=r_lane,\n",
        "                    heading_rel_line=theta,\n",
        "                    heading_gate=gate,\n",
        "                    r_speed=r_speed,\n",
        "                    v=v,\n",
        "                    v_tgt=v_tgt)\n",
        "\n",
        "        return obs, r, terminated, truncated, info\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5HHIyA6jrG8_"
      },
      "source": [
        "- [ ] Combine the updates you've already made to the Lane Keep / Cruise Control Rewards here\n",
        "- [ ] Optionally, you can explore additional reward terms (see below)\n",
        "\n",
        "#### ðŸ’¡ Further Improvements to reward:\n",
        "\n",
        "> (including but not limited to, feel free to innovate!)\n",
        "\n",
        "- Bonus for smooth driving (e.g., low jerk/acceleration changes)\n",
        "- Penalty for high steering angles at high speeds\n",
        "- Penalty for rapid deceleration/breaking\n",
        "- Bonus reward for progress along the lane\n",
        "- Termination penalty for going off-road\n",
        "- Bonus reward for finishing the track/road\n",
        "\n",
        "> See week 4 notebook (`ad_gym_vanilla_rl_v2025_08_19.ipynb`) for an example of some additional reward terms, such as termination penalties, progress rewards, quadratic rewards/penalties, etc.\n",
        "\n",
        "**Important Note** While adding additional reward terms can help guide the learning process, it's crucial to strike a balance. Overcomplicating the reward structure may lead to unintended behaviors or make it harder for the agent to learn effectively. Start with simple reward components that work well and iteratively refine them based on observed performance and learning outcomes.\n",
        "\n",
        "**Watch Out for Reward Exploitation!**:\n",
        "Certain rewards may be 'gamed' by the agent, leading to unintended behaviors. Some examples -\n",
        "- if a reward is given for progress along the lane, the agent might learn to exploit this by making rapid, erratic movements that maximize this reward without actually driving safely.\n",
        "- if a penalty is given for large steering angles, the agent might learn to drive very slowly to avoid making any significant turns, which could be impractical in real-world scenarios.\n",
        "- if a high reward is given for driving at a specified speed, the agent might learn to accelerate rapidly to that speed and then maintain it, potentially ignoring other important aspects of driving.\n",
        "- if a higher weight is given for speed reward, the agent might try to drive fast but in the wrong direction to maximize reward. or the agent might even drive in circles at high speed to maximize reward.\n",
        "\n",
        "Always monitor the agent's behavior and adjust the reward structure as necessary to ensure it aligns with desired outcomes.\n",
        "If you observe any cases of reward exploitation, that would also be a great learning experience and that deserves a mention in your report!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJeLNfLvrG8_"
      },
      "source": [
        "#### Initialise the Autonomous Driving Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IQTHAeOZrG8_"
      },
      "outputs": [],
      "source": [
        "# No internal controllers; agent controls both steering and throttle\n",
        "internal_policy_config_combined = {\n",
        "    \"enable_lane_keep\": False,\n",
        "    \"enable_cruise_control\": False,\n",
        "    \"action_interface\": \"full\",\n",
        "}\n",
        "\n",
        "env_comb = gym.make(\n",
        "    \"ai4rgym/autonomous_driving_env\",\n",
        "    render_mode=None,\n",
        "    bicycle_model_parameters=bicycle_model_parameters,\n",
        "    road_elements_list=road_elements_list,\n",
        "    numerical_integration_parameters=numerical_integration_parameters,\n",
        "    termination_parameters=termination_parameters,\n",
        "    initial_state_bounds=initial_state_bounds,\n",
        "    observation_parameters=observation_parameters,\n",
        "    internal_policy_config=internal_policy_config_combined,\n",
        ")\n",
        "\n",
        "env_comb = CombinedRewardWrapper(env_comb, k_distance=1.0, k_speed=1.0, target_speed_mps=60.0/3.6, use_recommended=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kA62utcMrG8_"
      },
      "source": [
        "#### Training RL Policy for Combined Lane Keeping and Cruise Control"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "loHQUvA1rG8_"
      },
      "outputs": [],
      "source": [
        "# Train a small PPO agent to verify the shaped reward works\n",
        "from stable_baselines3 import PPO\n",
        "import gymnasium as gym\n",
        "\n",
        "# Rescale the 1-D drive action to [-1, 1] for PPO stability\n",
        "env_comb_train = gym.wrappers.RescaleAction(\n",
        "    env_comb, min_action=-1, max_action=1)\n",
        "\n",
        "logdir = \"logs\"\n",
        "\n",
        "# Rename model/run for this experiment\n",
        "model_name = \"PPO_combined\"\n",
        "models_dir = f\"models/{model_name}\"\n",
        "figs_dir = f\"{models_dir}/figs\"\n",
        "ensure_dirs([models_dir, figs_dir])\n",
        "\n",
        "TIMESTEPS_PER_EPOCH = 50000\n",
        "\n",
        "# Build model and train using the same epoch/timestep structure\n",
        "model = PPO(\"MultiInputPolicy\", env_comb_train, verbose=1, tensorboard_log=logdir)\n",
        "\n",
        "start_epoch = 1\n",
        "N_EPOCHS_CC = 10  # follow previous pattern; adjust if desired\n",
        "\n",
        "for i in range(start_epoch, N_EPOCHS_CC + 1):\n",
        "    model.learn(\n",
        "        total_timesteps=TIMESTEPS_PER_EPOCH,\n",
        "        reset_num_timesteps=False,\n",
        "        tb_log_name=f\"{model_name}\",\n",
        "    )\n",
        "    step_count = TIMESTEPS_PER_EPOCH * i\n",
        "    model.save(f\"{models_dir}/{step_count}\")\n",
        "    eval_model(env_comb_train, model, figs_dir, step_count, plot_velocity=True)\n",
        "    print(f\"[CC] Finished epoch {i}/{N_EPOCHS_CC} | saved at step {step_count}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXfT6M-HrG9A"
      },
      "source": [
        "## Part 2 - Domain Randomization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yD_suI8CrG9A"
      },
      "outputs": [],
      "source": [
        "internal_policy_config = {\n",
        "    \"enable_lane_keep\": False,\n",
        "    \"enable_cruise_control\": False,\n",
        "    \"action_interface\": \"full\",\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SY09JfOhrG9A"
      },
      "source": [
        "Helper function for environment creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rdPFKmdarG9A"
      },
      "outputs": [],
      "source": [
        "def create_env(road_elements_list):\n",
        "    env = gym.make(\n",
        "        \"ai4rgym/autonomous_driving_env\",\n",
        "        render_mode=None,\n",
        "        bicycle_model_parameters=bicycle_model_parameters,\n",
        "        road_elements_list=road_elements_list,\n",
        "        numerical_integration_parameters=numerical_integration_parameters,\n",
        "        termination_parameters=termination_parameters,\n",
        "        initial_state_bounds=initial_state_bounds,\n",
        "        observation_parameters=observation_parameters,\n",
        "        internal_policy_config=internal_policy_config,\n",
        "    )\n",
        "\n",
        "    # Set the integration method and time step\n",
        "    Ts_sim = 0.05\n",
        "    integration_method = \"rk4\"\n",
        "    env.unwrapped.set_integration_method(integration_method)\n",
        "    env.unwrapped.set_integration_Ts(Ts_sim)\n",
        "\n",
        "    # Set the road condition\n",
        "    env.unwrapped.set_road_condition(road_condition=\"wet\")\n",
        "\n",
        "    # Rescale actions and wrap the environment\n",
        "    env = gym.wrappers.RescaleAction(env, min_action=-1, max_action=1)\n",
        "    env = CombinedRewardWrapper(env)\n",
        "    return env"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLuboBbJrG9A"
      },
      "source": [
        "### Domain Randomization Wrapper:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UjGq54WUrG9A"
      },
      "outputs": [],
      "source": [
        "from ai4rgym.envs.road import Road\n",
        "import random\n",
        "\n",
        "class DomainRandomizationWrapper(gym.Wrapper):\n",
        "    def __init__(self, env, road_randomization_params=None):\n",
        "        super(DomainRandomizationWrapper, self).__init__(env)\n",
        "        self.road_randomization_params = road_randomization_params\n",
        "\n",
        "    def generate_random_road_elements_list(self):\n",
        "        params = self.road_randomization_params or {}\n",
        "        num_elements_range = params.get('num_elements_range', (2, 5))\n",
        "        straight_length_range = params.get('straight_length_range', (50.0, 200.0))\n",
        "        curvature_range = params.get('curvature_range', (-1/500.0, 1/500.0))\n",
        "        angle_range = params.get('angle_range', (10.0, 60.0))\n",
        "\n",
        "        road_elements = []\n",
        "        num_elements = random.randint(*num_elements_range)\n",
        "\n",
        "        for _ in range(num_elements):\n",
        "            element_type = random.choice(['straight', 'curved'])\n",
        "            if element_type == 'straight':\n",
        "                length = random.uniform(*straight_length_range)\n",
        "                road_elements.append({\"type\": \"straight\", \"length\": length})\n",
        "            else:\n",
        "                curvature = random.uniform(*curvature_range)\n",
        "                angle = random.uniform(*angle_range)\n",
        "                road_elements.append({\n",
        "                    \"type\": \"curved\",\n",
        "                    \"curvature\": curvature,\n",
        "                    \"angle_in_degrees\": angle\n",
        "                })\n",
        "        return road_elements\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        # Generate a new random road\n",
        "        self.unwrapped.road_elements_list = self.generate_random_road_elements_list()\n",
        "        self.unwrapped.road = Road(epsilon_c=(1/10000), road_elements_list=self.unwrapped.road_elements_list)\n",
        "        self.unwrapped.total_road_length = self.unwrapped.road.get_total_length()\n",
        "        self.unwrapped.total_road_length_for_termination = max(self.unwrapped.total_road_length - 0.1, 0.9999 * self.unwrapped.total_road_length)\n",
        "        # Call the original reset method\n",
        "        return self.env.reset(**kwargs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZD0oU0fFrG9A"
      },
      "source": [
        "### Visualizing Random Roads:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N8x5Zye_rG9A"
      },
      "outputs": [],
      "source": [
        "# Example 1: Standard Randomization\n",
        "# road_randomization_params = {\n",
        "#     'num_elements_range': (3, 6),          # Road will have 3 to 6 elements\n",
        "#     'straight_length_range': (100.0, 150.0), # Length of straight segments between 100 and 150 meters\n",
        "#     'curvature_range': (-1/300.0, 1/300.0), # Curvature between -1/300 and 1/300 for curves\n",
        "#     'angle_range': (20.0, 45.0)            # Angle of the curves between 20 and 45 degrees\n",
        "# }\n",
        "\n",
        "# # Example 2: More Straight Roads\n",
        "# road_randomization_params = {\n",
        "#     'num_elements_range': (2, 4),            # Fewer elements, 2 to 4\n",
        "#     'straight_length_range': (150.0, 300.0), # Longer straight segments between 150 and 300 meters\n",
        "#     'curvature_range': (-1/1000.0, 1/1000.0), # Small curvature for gentler curves\n",
        "#     'angle_range': (10.0, 30.0)              # Curves have lower angles between 10 and 30 degrees\n",
        "# }\n",
        "\n",
        "# # Example 3: More challenging roads with Tighter curves\n",
        "# road_randomization_params = {\n",
        "#     'num_elements_range': (4, 8),             # More road elements\n",
        "#     'straight_length_range': (50.0, 100.0),   # Shorter straight segments\n",
        "#     'curvature_range': (-1/100.0, 1/100.0),   # Tighter curves\n",
        "#     'angle_range': (30.0, 70.0)               # More aggressive curves with wider angle range\n",
        "# }\n",
        "\n",
        "# Example 4: Completely Randomized\n",
        "road_randomization_params = {\n",
        "    'num_elements_range': (1, 10),             # Highly variable number of road elements\n",
        "    'straight_length_range': (50.0, 300.0),    # Straight segments can be very short or long\n",
        "    'curvature_range': (-1/50.0, 1/50.0),      # Very tight curves possible\n",
        "    'angle_range': (10.0, 90.0)                # Wide range of angles for curves\n",
        "}\n",
        "\n",
        "random_env = create_env(road_elements_list = road_elements_list, ) # An initial road_elements_list is required but will be overwritten\n",
        "random_env = DomainRandomizationWrapper(random_env, road_randomization_params=road_randomization_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- [ ] Define your own / Choose a different road randomization parameter"
      ],
      "metadata": {
        "id": "pwRmZzNsvwPZ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gw4V7EEGrG9A"
      },
      "source": [
        "Reset the random env - this will create a new random road"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oYnNYQkQrG9A"
      },
      "outputs": [],
      "source": [
        "random_env.reset()\n",
        "\n",
        "# Render the road\n",
        "random_env.unwrapped.render_matplotlib_init_figure()\n",
        "random_env.unwrapped.render_matplotlib_plot_road()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ocmJSAglrG9A"
      },
      "source": [
        "### Setting up Training/Evaluation Environments:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7-Wo6pMrrG9A"
      },
      "outputs": [],
      "source": [
        "### ------------- MAKE CHANGES TO THE ROAD RANDOMIZATION PARAMS HERE ------- ###\n",
        "road_randomization_params = None    # Use None for default randomization params\n",
        "# road_randomization_params = {\n",
        "#     \"num_elements_range\": (0, 0),\n",
        "#     \"straight_length_range\": (0.0, 0.0),\n",
        "#     \"curvature_range\": (-0/100.0, 0/100.0),\n",
        "#     \"angle_range\": (0.0, 0.0),\n",
        "# }\n",
        "\n",
        "### ------------------------------------------------------------------------ ###\n",
        "\n",
        "# Create the training environment with domain randomization\n",
        "train_env = create_env(road_elements_list=road_elements_list)  # Road elements will be randomized\n",
        "train_env = DomainRandomizationWrapper(train_env, road_randomization_params = road_randomization_params)\n",
        "train_env = CombinedRewardWrapper(train_env)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbVvbpkVrG9A"
      },
      "source": [
        "Difficult road for evaluation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JUn--ULgrG9A"
      },
      "outputs": [],
      "source": [
        "# Define a fixed, difficult road for evaluation\n",
        "road_elements_list_difficult = [\n",
        "    {\"type\": \"straight\", \"length\": 50.0, 'v_max_kph': 60.0},\n",
        "    {\"type\": \"curved\", \"curvature\": 1/300.0, \"angle_in_degrees\": 90.0, 'v_max_kph': 40.0},\n",
        "    {\"type\": \"straight\", \"length\": 50.0, 'v_max_kph': 30.0, 'v_max_kph': 50.0},\n",
        "    {\"type\": \"curved\", \"curvature\": -1/200.0, \"angle_in_degrees\": 120.0, 'v_max_kph': 40.0},\n",
        "    {\"type\": \"straight\", \"length\": 50.0, 'v_max_kph': 60.0},\n",
        "]\n",
        "\n",
        "# Create the evaluation environment with the fixed difficult road\n",
        "eval_env = create_env(road_elements_list_difficult)\n",
        "\n",
        "# Render the road\n",
        "eval_env.unwrapped.render_matplotlib_init_figure()\n",
        "eval_env.unwrapped.render_matplotlib_plot_road()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzWPpfo7rG9A"
      },
      "source": [
        "\n",
        "### Model Definition and Training:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lRse-rN0rG9A"
      },
      "outputs": [],
      "source": [
        "from stable_baselines3 import PPO, SAC, DDPG\n",
        "\n",
        "model_name = \"PPO_dr\"\n",
        "\n",
        "TIMESTEPS_PER_EPOCH = 50000\n",
        "EPOCHS = 20\n",
        "\n",
        "logdir = \"logs\"\n",
        "models_dir = f\"models/{model_name}\"\n",
        "figs_dir_train = f\"models/{model_name}/figs_train\"\n",
        "figs_dir_test = f\"models/{model_name}/figs_test\"\n",
        "\n",
        "ensure_dirs([logdir, models_dir, figs_dir_train, figs_dir_test])\n",
        "\n",
        "model = PPO(\"MultiInputPolicy\", train_env, verbose = 1, tensorboard_log=logdir)\n",
        "\n",
        "for i in range(1, EPOCHS+1):\n",
        "    model.learn(\n",
        "        total_timesteps=TIMESTEPS_PER_EPOCH,\n",
        "        reset_num_timesteps=False,\n",
        "        tb_log_name=f\"{model_name}\"\n",
        "    )\n",
        "    model.save(f\"{models_dir}/{TIMESTEPS_PER_EPOCH * i}\")\n",
        "    # Evaluate on the most recent training road\n",
        "    eval_model(train_env, model, figs_dir_train, TIMESTEPS_PER_EPOCH * i, plot_velocity=True)\n",
        "    # Evaluate on evaluation road\n",
        "    eval_model(eval_env, model, figs_dir_test, TIMESTEPS_PER_EPOCH * i, plot_velocity=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27Eo9xZerG9A"
      },
      "source": [
        "### Evaluating the RL Policy:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4KkcDg7rG9A"
      },
      "source": [
        "#### Load an RL model from the training loop:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b6ahQE4yrG9A"
      },
      "outputs": [],
      "source": [
        "model_name = \"PPO_dr\"\n",
        "model_idx = 950000\n",
        "\n",
        "path_for_saving_figures = f\"models/{model_name}/eval\"\n",
        "ensure_dir(path_for_saving_figures)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lid-_kvbrG9A"
      },
      "source": [
        "#### Put the RL model into a policy class with a 'standardized' interface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qX-zzdaDrG9A"
      },
      "outputs": [],
      "source": [
        "from policies.rl_policy import RLPolicy\n",
        "\n",
        "# Put the RL model into a policy class\n",
        "rl_policy = RLPolicy(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "csCN07-zrG9A"
      },
      "source": [
        "#### Perform One simulation of the policy, plot the time series results:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rTBogVNJrG9A"
      },
      "outputs": [],
      "source": [
        "# Import the function for simulating the autonomous driving environment\n",
        "from evaluation.evaluation_for_autonomous_driving import simulate_policy\n",
        "from evaluation.evaluation_for_autonomous_driving import plot_results_from_time_series_dict\n",
        "\n",
        "# Specify the length of the simulation in time steps\n",
        "# > If termination or truncation is flagged by the \"step\" function,\n",
        "#   Then, the simulation ends.\n",
        "N_sim = 5000\n",
        "\n",
        "# Specify the seed for when the simulate function resets the random number generator\n",
        "sim_seed = 1;\n",
        "\n",
        "# Call the function for simulating a given RL model\n",
        "sim_time_series_dict = simulate_policy(eval_env, N_sim, rl_policy, seed=sim_seed, verbose=1)\n",
        "\n",
        "# Call the plotting function\n",
        "file_name_suffix = \"example\"\n",
        "plot_details_list = plot_results_from_time_series_dict(eval_env, sim_time_series_dict, path_for_saving_figures, file_name_suffix, should_plot_reward=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hYPFU6Y-rG9A"
      },
      "source": [
        "#### Animate the time series results:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dxguAJodrG9A"
      },
      "outputs": [],
      "source": [
        "def animate_from_sim_time_series_dict(sim_time_series_dict, Ts, path_for_saving_figures):\n",
        "    # Extract the necessary trajectory information from the \"sim_time_series_dict\"\n",
        "    px_traj    = sim_time_series_dict[\"px\"]\n",
        "    py_traj    = sim_time_series_dict[\"py\"]\n",
        "    theta_traj = sim_time_series_dict[\"theta\"]\n",
        "    delta_traj = sim_time_series_dict[\"delta\"]\n",
        "    # Call the environments function to create the simulation\n",
        "    ani = eval_env.unwrapped.render_matplotlib_animation_of_trajectory(px_traj, py_traj, theta_traj, delta_traj, Ts, traj_increment=3)\n",
        "    # Save the animation\n",
        "    ani.save(f\"{path_for_saving_figures}/trajectory_animation.gif\")\n",
        "    print(f'Saved animation at {path_for_saving_figures}/trajectory_animation.gif')\n",
        "    # Return the animation object\n",
        "    return ani"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AF9fb66BrG9A"
      },
      "outputs": [],
      "source": [
        "# Call the animation functoin\n",
        "ani = animate_from_sim_time_series_dict(sim_time_series_dict, numerical_integration_parameters[\"Ts\"], path_for_saving_figures)\n",
        "# Display the animation\n",
        "from IPython.display import HTML\n",
        "HTML(ani.to_jshtml())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mcVA_E0nrG9B"
      },
      "source": [
        "### Performance Metrics:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3G4hgZa_rG9B"
      },
      "source": [
        "#### Define a performance Metrics Function per simulation time series:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lQcNNcMqrG9B"
      },
      "outputs": [],
      "source": [
        "def compute_performance_metrics_from_time_series(sim_time_series_dict):\n",
        "    # Compute the statistics of the distance to the line\n",
        "    # > Note that this is signed value, hence need to take absolute value of the data.\n",
        "    abs_dist_to_line_time_series = np.abs(sim_time_series_dict[\"distance_to_closest_point\"])\n",
        "    avg_dist  = np.nanmean(abs_dist_to_line_time_series)\n",
        "    std_dist  = np.nanstd(abs_dist_to_line_time_series)\n",
        "    max_dist  = np.nanmax(abs_dist_to_line_time_series)\n",
        "\n",
        "    # Compute the statistics of the speed in the forward direction (i.e., the body-frame x-axis direction)\n",
        "    speed_time_series = np.abs(sim_time_series_dict[\"vx\"])\n",
        "    avg_speed = np.nanmean(speed_time_series)\n",
        "    std_speed = np.nanstd(speed_time_series)\n",
        "    max_speed = np.nanmax(speed_time_series)\n",
        "    min_speed = np.nanmin(speed_time_series)\n",
        "\n",
        "    # Return the results\n",
        "    return {\n",
        "        \"avg_dist\"   :  avg_dist,\n",
        "        \"std_dist\"   :  std_dist,\n",
        "        \"max_dist\"   :  max_dist,\n",
        "        \"avg_speed\"  :  avg_speed * 3.6,\n",
        "        \"std_speed\"  :  std_speed * 3.6,\n",
        "        \"max_speed\"  :  max_speed * 3.6,\n",
        "        \"min_speed\"  :  min_speed * 3.6,\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "122hqzXzrG9B"
      },
      "source": [
        "#### Run the Performance Metric Function for one simulation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qHN6T9CkrG9B"
      },
      "outputs": [],
      "source": [
        "# Simulate with the same seed as above to check that it gets the results consistent\n",
        "sim_seed = 1;\n",
        "sim_time_series_dict = simulate_policy(eval_env, N_sim, rl_policy, seed=sim_seed, verbose=1)\n",
        "\n",
        "# Call the function for computing the performance metrics\n",
        "pm_dict = compute_performance_metrics_from_time_series(sim_time_series_dict)\n",
        "\n",
        "# Display the performance metric values\n",
        "print(\"Performance Metric dictionary:\")\n",
        "print(pm_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ko3EJs_brG9B"
      },
      "source": [
        "**NOTE:**\n",
        "> Please refer to the notebook `ad_gym_rl_reward_shaping_v2025_08_25.ipynb` for more details on evaluation and performance metrics."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "RL2",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}