{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_b2dNRBjADs"
      },
      "source": [
        "## Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kcdeMTmr1l-S"
      },
      "outputs": [],
      "source": [
        "!pip install -q gymnasium stable_baselines3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4qASuf1o1IFk"
      },
      "outputs": [],
      "source": [
        "!git clone https://gitlab.unimelb.edu.au/ai4r/ai4r-gym.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vioqx1Rb1_PY"
      },
      "outputs": [],
      "source": [
        "%cd ai4r-gym"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yFKScohkPqn6"
      },
      "source": [
        "## Library Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vMrhAAlE2ila"
      },
      "outputs": [],
      "source": [
        "import ai4rgym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import gymnasium as gym\n",
        "from stable_baselines3 import PPO, SAC, DDPG\n",
        "from utils import ensure_dir, ensure_dirs, eval_model, evaluate_policy\n",
        "from utils import plot_rewards, plot_and_animate_trajectory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rcgPKmEPPnCS"
      },
      "source": [
        "## Environment Settings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KdUyqNCx2dvg"
      },
      "source": [
        "### Specify the Vehicle Parameters\n",
        "\n",
        "Dictionary with car specifications,\n",
        "in the form of a dynamic bicycle model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PFXr8MMT2C3Y"
      },
      "outputs": [],
      "source": [
        "# SPECIFY THE VEHCILE PARAMETERS\n",
        "bicycle_model_parameters = {\n",
        "    \"Lf\" : 0.55*2.875,\n",
        "    \"Lr\" : 0.45*2.875,\n",
        "    \"m\"  : 2000.0,\n",
        "    \"Iz\" : (1.0/12.0) * 2000.0 * (4.692**2+1.850**2),\n",
        "    \"Cm\" : (1.0/100.0) * (1.0 * 400.0 * 9.0) / 0.2286,\n",
        "    \"Cd\" : 0.5 * 0.24 * 2.2204 * 1.202,\n",
        "    \"delta_offset\" : 0 * np.pi/180,\n",
        "    \"delta_request_max\" : 45 * np.pi/180,\n",
        "    \"Ddelta_lower_limit\" : -45 * np.pi/180,\n",
        "    \"Ddelta_upper_limit\" :  45 * np.pi/180,\n",
        "    \"v_transition_min\" : 500.0 / 3.6,\n",
        "    \"v_transition_max\" : 600.0 / 3.6,\n",
        "    \"body_len_f\" : (0.55*2.875) * 1.5,\n",
        "    \"body_len_r\" : (0.45*2.875) * 1.5,\n",
        "    \"body_width\" : 2.50,\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZvLBp0Y2oca"
      },
      "source": [
        "### Specify the Road\n",
        "\n",
        "\n",
        "Specified as a list of dictionaries, where each\n",
        "element in the list specifies a segment of the road.\n",
        "Example segment dictionaries:\n",
        "```Python\n",
        "{\"type\":\"straight\", \"length\":3.0}\n",
        "{\"type\":\"curved\", \"curvature\":1/50.0, \"angle_in_degrees\":45.0}\n",
        "{\"type\":\"curved\", \"curvature\":1/50.0, \"length\":30.0}\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TCXqw6rF2Cz1"
      },
      "outputs": [],
      "source": [
        "road_elements_list = [\n",
        "    {\"type\":\"straight\", \"length\":100.0, \"v_max_kph\":60.0},\n",
        "    {\"type\":\"curved\", \"curvature\":1/800.0, \"angle_in_degrees\":15.0, \"v_max_kph\":50.0},\n",
        "    {\"type\":\"straight\", \"length\":100.0, \"v_max_kph\":60.0},\n",
        "    {\"type\":\"curved\", \"curvature\":-1/400.0, \"angle_in_degrees\":30.0, \"v_max_kph\":40.0},\n",
        "    {\"type\":\"straight\", \"length\":100.0, \"v_max_kph\":60.0},\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-Fn5qyh2r69"
      },
      "source": [
        "### Specify the Numerical Integration Details\n",
        "\n",
        "The options available for the numerical\n",
        "integration method are:\n",
        "`[\"euler\", \"huen\", \"midpoint\", \"rk4\", \"rk45\"]`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2e5yTWLP2CwX"
      },
      "outputs": [],
      "source": [
        "numerical_integration_parameters = {\n",
        "    \"method\" : \"rk4\",\n",
        "    \"Ts\" : 0.05,\n",
        "    \"num_steps_per_Ts\" : 1,\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4j-7ygm2xLw"
      },
      "source": [
        "###  Specify the Initial State Distribution\n",
        "\n",
        "The initial state is sampled from a uniform\n",
        "distribution between the minimum and maximum\n",
        "(i.e., between lower and upper bounds)\n",
        "> Note: a factor of (1/3.6) converts from units of [km/h] to [m/s]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xFOpyqMd2xkG"
      },
      "outputs": [],
      "source": [
        "py_init_min = -1.0\n",
        "py_init_max =  1.0\n",
        "\n",
        "v_init_min_in_kmh = 55.0\n",
        "v_init_max_in_kmh = 65.0\n",
        "\n",
        "py_init_min = -1.0\n",
        "py_init_max =  1.0\n",
        "\n",
        "v_init_min_in_kmh = 55.0\n",
        "v_init_max_in_kmh = 65.0\n",
        "\n",
        "initial_state_bounds = {\n",
        "    \"px_init_min\" : 0.0,\n",
        "    \"px_init_max\" : 0.0,\n",
        "    \"py_init_min\" : py_init_min,\n",
        "    \"py_init_max\" : py_init_max,\n",
        "    \"theta_init_min\" : 0.0,\n",
        "    \"theta_init_max\" : 0.0,\n",
        "    \"vx_init_min\" : v_init_min_in_kmh * (1.0/3.6),\n",
        "    \"vx_init_max\" : v_init_max_in_kmh * (1.0/3.6),\n",
        "    \"vy_init_min\" : 0.0,\n",
        "    \"vy_init_max\" : 0.0,\n",
        "    \"omega_init_min\" : 0.0,\n",
        "    \"omega_init_max\" : 0.0,\n",
        "    \"delta_init_min\" : 0.0,\n",
        "    \"delta_init_max\" : 0.0,\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UB-aBfdd20iz"
      },
      "source": [
        "### Specify the Observation Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WdTlLQE36PUX"
      },
      "outputs": [],
      "source": [
        "# SPECIFY THE OBSERVATION PARAMETERS\n",
        "observation_parameters = {\n",
        "    \"should_include_ground_truth_px\"                       :  \"info\",\n",
        "    \"should_include_ground_truth_py\"                       :  \"info\",\n",
        "    \"should_include_ground_truth_theta\"                    :  \"info\",\n",
        "    \"should_include_ground_truth_vx\"                       :  \"info\",\n",
        "    \"should_include_ground_truth_vy\"                       :  \"info\",\n",
        "    \"should_include_ground_truth_omega\"                    :  \"info\",\n",
        "    \"should_include_ground_truth_delta\"                    :  \"info\",\n",
        "    \"should_include_road_progress_at_closest_point\"        :  \"info\",\n",
        "    \"should_include_vx_sensor\"                             :  \"info\",\n",
        "    \"should_include_distance_to_closest_point\"             :  \"obs\",\n",
        "    \"should_include_heading_angle_relative_to_line\"        :  \"obs\",\n",
        "    \"should_include_heading_angular_rate_gyro\"             :  \"info\",\n",
        "    \"should_include_closest_point_coords_in_body_frame\"    :  \"info\",\n",
        "    \"should_include_look_ahead_line_coords_in_body_frame\"  :  \"info\",\n",
        "    \"should_include_road_curvature_at_closest_point\"       :  \"obs\",\n",
        "    \"should_include_look_ahead_road_curvatures\"            :  \"info\",\n",
        "    \"should_include_speed_limit\"                           :  \"obs\",\n",
        "    \"should_include_recommended_speed\"                     :  \"obs\",\n",
        "\n",
        "\n",
        "    \"scaling_for_ground_truth_px\"                       :  1.0,\n",
        "    \"scaling_for_ground_truth_py\"                       :  1.0,\n",
        "    \"scaling_for_ground_truth_theta\"                    :  1.0,\n",
        "    \"scaling_for_ground_truth_vx\"                       :  1.0,\n",
        "    \"scaling_for_ground_truth_vy\"                       :  1.0,\n",
        "    \"scaling_for_ground_truth_omega\"                    :  1.0,\n",
        "    \"scaling_for_ground_truth_delta\"                    :  1.0,\n",
        "    \"scaling_for_road_progress_at_closest_point\"        :  1.0,\n",
        "    \"scaling_for_vx_sensor\"                             :  1.0,\n",
        "    \"scaling_for_distance_to_closest_point\"             :  1.0,\n",
        "    \"scaling_for_heading_angle_relative_to_line\"        :  1.0,\n",
        "    \"scaling_for_heading_angular_rate_gyro\"             :  1.0,\n",
        "    \"scaling_for_closest_point_coords_in_body_frame\"    :  1.0,\n",
        "    \"scaling_for_look_ahead_line_coords_in_body_frame\"  :  1.0,\n",
        "    \"scaling_for_road_curvature_at_closest_point\"       :  1.0,\n",
        "    \"scaling_for_look_ahead_road_curvatures\"            :  1.0,\n",
        "    \"scaling_for_speed_limit\"                           :  1.0,\n",
        "    \"scaling_for_recommended_speed\"                     :  1.0,\n",
        "\n",
        "    \"vx_sensor_bias\"    : 0.0,\n",
        "    \"vx_sensor_stddev\"  : 0.1,\n",
        "\n",
        "    \"distance_to_closest_point_bias\"    :  0.0,\n",
        "    \"distance_to_closest_point_stddev\"  :  0.01,\n",
        "\n",
        "    \"heading_angle_relative_to_line_bias\"    :  0.0,\n",
        "    \"heading_angle_relative_to_line_stddev\"  :  0.01,\n",
        "\n",
        "    \"heading_angular_rate_gyro_bias\"    :  0.0,\n",
        "    \"heading_angular_rate_gyro_stddev\"  :  0.01,\n",
        "\n",
        "    \"closest_point_coords_in_body_frame_bias\"    :  0.0,\n",
        "    \"closest_point_coords_in_body_frame_stddev\"  :  0.0,\n",
        "\n",
        "    \"look_ahead_line_coords_in_body_frame_bias\"    :  0.0,\n",
        "    \"look_ahead_line_coords_in_body_frame_stddev\"  :  0.0,\n",
        "\n",
        "    \"road_curvature_at_closest_point_bias\"    :  0.0,\n",
        "    \"road_curvature_at_closest_point_stddev\"  :  0.0,\n",
        "\n",
        "    \"look_ahead_road_curvatures_bias\"    :  0.0,\n",
        "    \"look_ahead_road_curvatures_stddev\"  :  0.0,\n",
        "\n",
        "    \"look_ahead_line_coords_in_body_frame_distance\"    :  100.0,\n",
        "    \"look_ahead_line_coords_in_body_frame_num_points\"  :  10,\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3d4y71xEJZP"
      },
      "source": [
        "### Specify Termination Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pLnYuvG9EJKC"
      },
      "outputs": [],
      "source": [
        "termination_parameters = {\n",
        "    \"speed_lower_bound\"  :  0.0,\n",
        "    \"speed_upper_bound\"  :  (200.0/3.6),\n",
        "    \"distance_to_closest_point_upper_bound\"  :  20.0,\n",
        "    \"reward_speed_lower_bound\"  :  0.0,\n",
        "    \"reward_speed_upper_bound\"  :  0.0,\n",
        "    \"reward_distance_to_closest_point_upper_bound\"  :  0.0,\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gvm3nDWDQo_S"
      },
      "source": [
        "## Environment Definition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvkR3qZm27ua"
      },
      "source": [
        "### Initialise the Autonomous Driving Environment\n",
        "\n",
        "Options available for the \"render_mode\" are:\n",
        "`[\"matplotlib\", None]`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tCAOQ0AE25iF"
      },
      "outputs": [],
      "source": [
        "env = gym.make(\n",
        "    \"ai4rgym/autonomous_driving_env\",\n",
        "    render_mode=None,\n",
        "    bicycle_model_parameters=bicycle_model_parameters,\n",
        "    road_elements_list=road_elements_list,\n",
        "    numerical_integration_parameters=numerical_integration_parameters,\n",
        "    termination_parameters=termination_parameters,\n",
        "    initial_state_bounds=initial_state_bounds,\n",
        "    observation_parameters=observation_parameters,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhoYSGEW3AJ1"
      },
      "source": [
        "### Additional Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qPVqdwYT29qR"
      },
      "outputs": [],
      "source": [
        "# > Time increment per simulation step (units: seconds)\n",
        "Ts_sim = 0.05\n",
        "\n",
        "# Specify the integration method to simulate\n",
        "integration_method = \"rk4\"\n",
        "\n",
        "# Set the integration method and Ts of the gymnasium\n",
        "env.unwrapped.set_integration_method(integration_method)\n",
        "env.unwrapped.set_integration_Ts(Ts_sim)\n",
        "# Set the road condition\n",
        "env.unwrapped.set_road_condition(road_condition=\"wet\")\n",
        "\n",
        "env = gym.wrappers.RescaleAction(env, min_action=-1, max_action=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqEiUpY5QuTj"
      },
      "source": [
        "### Verify Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3gdnT5i33Bhw"
      },
      "outputs": [],
      "source": [
        "# Reset the gymnasium\n",
        "# > which also returns the first observation\n",
        "observation, info_dict = env.reset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zchNh_TL3DOE"
      },
      "outputs": [],
      "source": [
        "observation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eI-M55-z3EPz"
      },
      "outputs": [],
      "source": [
        "info_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VGAzcpLL3GfO"
      },
      "outputs": [],
      "source": [
        "env.observation_space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "34cB5qYp7SZG"
      },
      "outputs": [],
      "source": [
        "env.action_space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wVyWdy607UTe"
      },
      "outputs": [],
      "source": [
        "random_action = env.action_space.sample()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "guAJPyam7ZiE"
      },
      "outputs": [],
      "source": [
        "observation, reward, terminated, truncated, info = env.step(random_action)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pcn-AKpv7dWj"
      },
      "outputs": [],
      "source": [
        "observation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xaTK3udm7eBb"
      },
      "outputs": [],
      "source": [
        "reward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T6sVR-Cj7euG"
      },
      "outputs": [],
      "source": [
        "info"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9C8Ub1D8nNB"
      },
      "source": [
        "### Reward Wrapper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OGlRTmON8mcL"
      },
      "outputs": [],
      "source": [
        "from typing import Optional\n",
        "\n",
        "\n",
        "class CustomRewardWrapper(gym.Wrapper):\n",
        "    \"\"\"\n",
        "    A custom wrapper that reconstructs the environment's previous reward logic\n",
        "    (progress, deviation from line, speed shaping, and termination rewards)\n",
        "    while the base environment returns zero reward. This keeps reward shaping\n",
        "    out of the env and makes it configurable/replaceable.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, env: gym.Env, cfg: Optional[dict] = None):\n",
        "        super().__init__(env)\n",
        "        self.cfg = cfg or {}\n",
        "        # Coefficients for previous env reward composition\n",
        "        # Previous env used: 0.0*progress + 100.0*deviation + 1.0*speed + termination\n",
        "        # Maintain same defaults here; allow override via cfg\n",
        "        self.k_progress = self.cfg.get(\"k_progress\", 0.0)\n",
        "        self.k_deviation = self.cfg.get(\"k_deviation\", 100.0)\n",
        "        self.k_speed = self.cfg.get(\"k_speed\", 1.0)\n",
        "        # Bonus when finished (not present previously, default 0)\n",
        "        self.finished_bonus = self.cfg.get(\"finished_bonus\", 0.0)\n",
        "        # Timeout penalty if a TimeLimit truncation occurs\n",
        "        self.timeout_penalty = self.cfg.get(\"timeout_penalty\", 0.0)\n",
        "\n",
        "    # Standalone re-implementation of previous reward shaping terms\n",
        "    @staticmethod\n",
        "    def reward_for_distance_to_line(d: float) -> float:\n",
        "        \"\"\"\n",
        "        Demo reward function for maintaining trajectory on the road center\n",
        "        Quadratic reward shaping around\n",
        "        - (high reward) a desired distance (1m) from the road center\n",
        "        - (medium reward) between 1m~2m from road center\n",
        "        Penalties for exceeding a distance of 2m from the road center\n",
        "        \"\"\"\n",
        "        a = 3.0\n",
        "        b = 1.0\n",
        "        c = 2.0\n",
        "        if d < 0.5:\n",
        "            return a * (1 - d**2)\n",
        "        elif 0.5 <= d < 2:\n",
        "            return b * (2 - d) ** 2\n",
        "        else:\n",
        "            return -c * (d - 2) ** 3\n",
        "\n",
        "    @staticmethod\n",
        "    def reward_for_speed(speed_in_kmph: float) -> float:\n",
        "        \"\"\"\n",
        "        Demo reward function for driving at a set speed\n",
        "        Quadratic reward shaping around a desired speed (60 km/h)\n",
        "        Linear penalty for exceeding a speed of 120 km/h\n",
        "        \"\"\"\n",
        "        a = 1.0 / 12.0\n",
        "        b = 300.0\n",
        "        e = 10.0\n",
        "        if 0 <= speed_in_kmph < 120:\n",
        "            return -a * (speed_in_kmph - 60) ** 2 + b\n",
        "        else:\n",
        "            return -e * (speed_in_kmph - 120)\n",
        "\n",
        "    def step(self, action):\n",
        "        obs, base_r, terminated, truncated, info = self.env.step(action)\n",
        "        # Base env reward is intentionally zero; we reconstruct externally\n",
        "        r = 0.0\n",
        "\n",
        "        # We need values used by the original env reward terms.\n",
        "        # These are available via env.current_ground_truth and the car state.\n",
        "        # Access safely; if missing, default to zeros.\n",
        "        base = self.env.unwrapped\n",
        "        gt = getattr(base, \"current_ground_truth\", {}) or {}\n",
        "        progress_at_closest_point = float(gt.get(\"road_progress_at_closest_point\", 0.0))\n",
        "        distance_to_closest_point = float(abs(gt.get(\"distance_to_closest_point\", 0.0)))\n",
        "\n",
        "        # The env used previous_progress_at_closest_p internally; in the wrapper\n",
        "        # we track our own previous progress to compute progress_reward.\n",
        "        if not hasattr(self, \"_prev_progress\"):\n",
        "            self._prev_progress = progress_at_closest_point\n",
        "        progress_reward = progress_at_closest_point - self._prev_progress\n",
        "        self._prev_progress = progress_at_closest_point\n",
        "\n",
        "        # Speed shaping (m/s) relative to road recommended speed (m/s)\n",
        "        vx_mps = float(getattr(getattr(base, \"car\", None), \"vx\", 0.0))\n",
        "        vx_kph = vx_mps * 3.6\n",
        "        recommended_speed_mps = float(gt.get(\"recommended_speed_at_closest_point\", 0.0))\n",
        "        speed_limit_mps = float(gt.get(\"speed_limit_at_closest_point\", 0.0))\n",
        "\n",
        "        deviation_reward = self.reward_for_distance_to_line(distance_to_closest_point)\n",
        "        speed_reward = self.reward_for_speed(vx_kph)\n",
        "        # speed_reward = self.reward_for_speed(vx_mps, recommended_speed_mps, speed_limit_mps)\n",
        "\n",
        "        r += self.k_progress * progress_reward\n",
        "        r += self.k_deviation * deviation_reward\n",
        "        r += self.k_speed * speed_reward\n",
        "\n",
        "        # Termination-based rewards/penalties\n",
        "        term = info.get(\"termination\", {}) if isinstance(info, dict) else {}\n",
        "        term_cfg = info.get(\"termination_rewards\", {}) if isinstance(info, dict) else {}\n",
        "        if terminated:\n",
        "            if term.get(\"speed_high\"):\n",
        "                r += float(term_cfg.get(\"speed_upper_bound\", 0.0))\n",
        "            if term.get(\"speed_low\"):\n",
        "                r += float(term_cfg.get(\"speed_lower_bound\", 0.0))\n",
        "            if term.get(\"off_track\"):\n",
        "                r += float(term_cfg.get(\"off_track\", 0.0))\n",
        "            if term.get(\"finished\"):\n",
        "                r += float(self.finished_bonus)\n",
        "\n",
        "        # Time-limit truncation (Gymnasium's TimeLimit sets this flag in info)\n",
        "        if truncated and isinstance(info, dict) and info.get(\"TimeLimit.truncated\", False):\n",
        "            r += float(self.timeout_penalty)\n",
        "\n",
        "        return obs, r, terminated, truncated, info\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        res = self.env.reset(**kwargs)\n",
        "        # Reset wrapper progress tracker\n",
        "        self._prev_progress = 0.0\n",
        "        # After reset, try to align with current env ground-truth if available\n",
        "        gt = getattr(self.env, \"current_ground_truth\", {}) or {}\n",
        "        self._prev_progress = float(gt.get(\"road_progress_at_closest_point\", 0.0))\n",
        "        return res"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "se7pfJLC9gVD"
      },
      "source": [
        "Create a new environment and wrap it with the Reward Wrapper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VPVyjOle9Wx-",
        "outputId": "f9d83fb1-78cb-4b61-dca2-4a716d4dec8e"
      },
      "outputs": [],
      "source": [
        "env = gym.make(\n",
        "    \"ai4rgym/autonomous_driving_env\",\n",
        "    render_mode=None,\n",
        "    bicycle_model_parameters=bicycle_model_parameters,\n",
        "    road_elements_list=road_elements_list,\n",
        "    numerical_integration_parameters=numerical_integration_parameters,\n",
        "    termination_parameters=termination_parameters,\n",
        "    initial_state_bounds=initial_state_bounds,\n",
        "    observation_parameters=observation_parameters,\n",
        ")\n",
        "\n",
        "# > Time increment per simulation step (units: seconds)\n",
        "Ts_sim = 0.05\n",
        "\n",
        "# Specify the integration method to simulate\n",
        "integration_method = \"rk4\"\n",
        "\n",
        "# Set the integration method and Ts of the gymnasium\n",
        "env.unwrapped.set_integration_method(integration_method)\n",
        "env.unwrapped.set_integration_Ts(Ts_sim)\n",
        "# Set the road condition\n",
        "env.unwrapped.set_road_condition(road_condition=\"wet\")\n",
        "\n",
        "env = gym.wrappers.RescaleAction(env, min_action=-1, max_action=1)\n",
        "\n",
        "cfg = { 'k_deviation': 100.0, 'k_speed': 1.0}\n",
        "env = CustomRewardWrapper(env, cfg=cfg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wg-Q9g7i9rok"
      },
      "source": [
        "Let's check the reward now:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RftxpP3B9qtD",
        "outputId": "0493c654-be4c-49b9-bca2-c538b8aa3ede"
      },
      "outputs": [],
      "source": [
        "observation, info = env.reset()\n",
        "random_action = env.action_space.sample()\n",
        "observation, reward, terminated, truncated, info = env.step(random_action)\n",
        "print(reward)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yo2aNmR3aF60"
      },
      "source": [
        "## Model Definition and Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CV13EDTOGlEU"
      },
      "outputs": [],
      "source": [
        "model_name = \"PPO\"\n",
        "\n",
        "TIMESTEPS_PER_EPOCH = 50000\n",
        "N_EPOCHS = 10\n",
        "\n",
        "logdir = \"logs\"\n",
        "models_dir = f\"models/{model_name}\"\n",
        "figs_dir = f\"models/{model_name}/figs\"\n",
        "\n",
        "ensure_dirs([logdir, models_dir, figs_dir])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DSIjEgi87neh",
        "outputId": "45829d8f-f307-4235-e916-ea17a87e286d"
      },
      "outputs": [],
      "source": [
        "model = PPO(\"MultiInputPolicy\", env, verbose = 1)\n",
        "#model = SAC(\"MultiInputPolicy\", env, verbose = 1)\n",
        "#model = DDPG(\"MultiInputPolicy\", env, verbose = 1)\n",
        "#model = TD3(\"MultiInputPolicy\", env, verbose = 1)\n",
        "#model = A2C(\"MultiInputPolicy\", env, verbose = 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YIwe4p1SPh6U"
      },
      "source": [
        "## Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pRLF9e17GtSl",
        "outputId": "c0eed871-9ba8-41a2-9648-5b32bb97b864"
      },
      "outputs": [],
      "source": [
        "model = PPO(\"MultiInputPolicy\", env, verbose = 1, tensorboard_log=logdir)\n",
        "epoch = 1\n",
        "N_EPOCHS = 20\n",
        "\n",
        "for i in range(epoch, N_EPOCHS + 1):\n",
        "    model.learn(\n",
        "        total_timesteps=TIMESTEPS_PER_EPOCH,\n",
        "        reset_num_timesteps=False,\n",
        "        tb_log_name=f\"{model_name}\"\n",
        "    )\n",
        "    model.save(f\"{models_dir}/{TIMESTEPS_PER_EPOCH * i}\")\n",
        "    eval_model(env, model, figs_dir, TIMESTEPS_PER_EPOCH * i)\n",
        "    epoch += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oo4LTUy97rHj"
      },
      "source": [
        "## Evaluating the Policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ywyd8QuoHOL1"
      },
      "outputs": [],
      "source": [
        "from evaluation.evaluation_for_autonomous_driving import simulate_policy, plot_results_from_time_series_dict\n",
        "from policies.rl_policy import RLPolicy\n",
        "import os\n",
        "\n",
        "def save_time_series_csv(path, filename, results):\n",
        "    os.makedirs(path, exist_ok=True)\n",
        "    fpath = os.path.join(path, filename)\n",
        "\n",
        "    # Build a simple CSV with key signals for tuning\n",
        "    headers = [\n",
        "        \"time_index\",\n",
        "        \"time_in_seconds\",\n",
        "        \"road_progress_at_closest_point\",\n",
        "        \"distance_to_closest_point\",\n",
        "        \"vx\",\n",
        "        \"drive_command\",\n",
        "        \"delta_request\",\n",
        "    ]\n",
        "\n",
        "    # The evaluation helper exposes arrays with length N+1\n",
        "    ti = np.asarray(results[\"time_index\"]).flatten()\n",
        "    ts = np.asarray(results[\"time_in_seconds\"]).flatten()\n",
        "    prog = np.asarray(results[\"road_progress_at_closest_point\"]).flatten()\n",
        "    dist = np.asarray(results[\"distance_to_closest_point\"]).flatten()\n",
        "    vx = np.asarray(results[\"vx\"]).flatten()\n",
        "    dc = np.asarray(results[\"drive_command\"]).flatten()\n",
        "    dr = np.asarray(results[\"delta_request\"]).flatten()\n",
        "\n",
        "    data = np.stack([ti, ts, prog, dist, vx, dc, dr], axis=1)\n",
        "\n",
        "    # Write CSV\n",
        "    with open(fpath, \"w\") as f:\n",
        "        f.write(\",\".join(headers) + \"\\n\")\n",
        "        np.savetxt(f, data, delimiter=\",\", fmt=\"%.6f\")\n",
        "    print(f\"Saved results CSV: {fpath}\")\n",
        "\n",
        "\n",
        "## -----------------\n",
        "#  SIMULATE RL MODEL\n",
        "#  -----------------\n",
        "def simulate_rl_model(env, N_sim, rl_model, seed=None, should_save_look_ahead_results=False, should_save_observations=False, verbose=0):\n",
        "    # Put the RL model into a policy class\n",
        "    rl_policy = RLPolicy(rl_model)\n",
        "    # Call the simluate policy function\n",
        "    sim_time_series_dict = simulate_policy(env, N_sim, rl_policy, seed, should_save_look_ahead_results, should_save_observations, verbose)\n",
        "    # Return the results dictionary\n",
        "    return sim_time_series_dict\n",
        "\n",
        "save_dir = \"models/PPO/eval\"\n",
        "ensure_dir(save_dir)\n",
        "\n",
        "N_sim = 1500\n",
        "results = simulate_rl_model(\n",
        "    env, N_sim, rl_model=model, seed=42, should_save_look_ahead_results=False, should_save_observations=True, verbose=1\n",
        ")\n",
        "\n",
        "# Save timeseries to CSV for tuning\n",
        "save_time_series_csv(save_dir, \"internal_policies_results.csv\", results)\n",
        "\n",
        "# Quick trajectory/time-series plots\n",
        "plot_results_from_time_series_dict(env, results, save_dir, file_name_suffix=\"internal_policies\", should_plot_reward=False)\n",
        "\n",
        "# Optional: animation (commented to keep example lightweight)\n",
        "# px, py = results[\"px\"], results[\"py\"]\n",
        "# theta, delta = results[\"theta\"], results[\"delta\"]\n",
        "# ani = env.unwrapped.render_matplotlib_animation_of_trajectory(px, py, theta, delta, numerical_integration_parameters[\"Ts\"], traj_increment=3)\n",
        "# ani.save(os.path.join(save_dir, \"internal_policies_animation.gif\"))\n",
        "\n",
        "# Print simple summary metrics\n",
        "total_progress = np.nanmax(results[\"road_progress_at_closest_point\"]) - np.nanmin(results[\"road_progress_at_closest_point\"])\n",
        "mean_abs_lat = np.nanmean(np.abs(results[\"distance_to_closest_point\"]))\n",
        "mean_speed = np.nanmean(results[\"vx\"]) * 3.6\n",
        "print(f\"Total progress along road: {total_progress:.2f} m\")\n",
        "print(f\"Mean |distance to line|: {mean_abs_lat:.3f} m\")\n",
        "print(f\"Mean speed: {mean_speed:.2f} km/h\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ZYG1m6yHOL1"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "RL",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
